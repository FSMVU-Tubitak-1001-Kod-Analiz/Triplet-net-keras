{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import random\n",
    "import matplotlib.patheffects as PathEffects\n",
    "\n",
    "from keras.layers import Input, Conv2D, Lambda, Dense, Flatten,MaxPooling2D, concatenate\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.losses import binary_crossentropy\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from itertools import permutations\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.0\n",
       "1      0.0\n",
       "2      0.0\n",
       "3      0.0\n",
       "4      0.0\n",
       "      ... \n",
       "95    14.0\n",
       "96    14.0\n",
       "97    14.0\n",
       "98    14.0\n",
       "99    14.0\n",
       "Name: smellKey_encoded, Length: 100, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('Data/sub_dataset_list.pkl', 'rb') as f:\n",
    "    smellKey_list = pickle.load(f)\n",
    "smellKey_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 768)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path = 'Data/fn_smell_embeddings_with_triplet.pt'\n",
    "loaded_ast_embeddings = torch.load(save_path)\n",
    "loaded_ast_embeddings = loaded_ast_embeddings.numpy()\n",
    "loaded_ast_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\n",
    "    'fn_smell_embeddings.pt': loaded_ast_embeddings.tolist(),\n",
    "    'smellKey_encoded': smellKey_list\n",
    "})\n",
    "\n",
    "X = np.array(data['fn_smell_embeddings.pt'].tolist())\n",
    "y = np.array(data['smellKey_encoded'])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 768)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 768)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 768)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our own plot function\n",
    "def scatter(x, labels, subtitle=None):\n",
    "    # We choose a color palette with seaborn.\n",
    "    palette = np.array(sns.color_palette(\"hls\", 10))\n",
    "\n",
    "    # We create a scatter plot.\n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40,\n",
    "                    c=palette[labels.astype(int)])\n",
    "    plt.xlim(-25, 25)\n",
    "    plt.ylim(-25, 25)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "\n",
    "    # We add the labels for each digit.\n",
    "    txts = []\n",
    "    for i in range(10):\n",
    "        # Position of each label.\n",
    "        xtext, ytext = np.median(x[labels == i, :], axis=0)\n",
    "        txt = ax.text(xtext, ytext, str(i), fontsize=24)\n",
    "        txt.set_path_effects([\n",
    "            PathEffects.Stroke(linewidth=5, foreground=\"w\"),\n",
    "            PathEffects.Normal()])\n",
    "        txts.append(txt)\n",
    "        \n",
    "    if subtitle != None:\n",
    "        plt.suptitle(subtitle)\n",
    "        \n",
    "    plt.savefig(subtitle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 768)\n",
      "(80, 768)\n"
     ]
    }
   ],
   "source": [
    "x_train_flat = x_train\n",
    "x_test_flat = x_test\n",
    "print(x_train.shape)\n",
    "print(x_train_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 10 is out of bounds for axis 0 with size 10",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\omerkerem.adali\\Desktop\\tubitak\\Triplet-net-keras\\Triplet NN Test on MNIST.ipynb Cell 11\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/omerkerem.adali/Desktop/tubitak/Triplet-net-keras/Triplet%20NN%20Test%20on%20MNIST.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tsne \u001b[39m=\u001b[39m TSNE()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/omerkerem.adali/Desktop/tubitak/Triplet-net-keras/Triplet%20NN%20Test%20on%20MNIST.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train_tsne_embeds \u001b[39m=\u001b[39m tsne\u001b[39m.\u001b[39mfit_transform(x_train[:\u001b[39m512\u001b[39m])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/omerkerem.adali/Desktop/tubitak/Triplet-net-keras/Triplet%20NN%20Test%20on%20MNIST.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m scatter(train_tsne_embeds, y_train[:\u001b[39m512\u001b[39;49m], \u001b[39m\"\u001b[39;49m\u001b[39mSamples from Training Data\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/omerkerem.adali/Desktop/tubitak/Triplet-net-keras/Triplet%20NN%20Test%20on%20MNIST.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m eval_tsne_embeds \u001b[39m=\u001b[39m tsne\u001b[39m.\u001b[39mfit_transform(x_test[:\u001b[39m512\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/omerkerem.adali/Desktop/tubitak/Triplet-net-keras/Triplet%20NN%20Test%20on%20MNIST.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m scatter(eval_tsne_embeds, y_test[:\u001b[39m512\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mSamples from Validation Data\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\omerkerem.adali\\Desktop\\tubitak\\Triplet-net-keras\\Triplet NN Test on MNIST.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/omerkerem.adali/Desktop/tubitak/Triplet-net-keras/Triplet%20NN%20Test%20on%20MNIST.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m f \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/omerkerem.adali/Desktop/tubitak/Triplet-net-keras/Triplet%20NN%20Test%20on%20MNIST.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplot(aspect\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mequal\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/omerkerem.adali/Desktop/tubitak/Triplet-net-keras/Triplet%20NN%20Test%20on%20MNIST.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m sc \u001b[39m=\u001b[39m ax\u001b[39m.\u001b[39mscatter(x[:,\u001b[39m0\u001b[39m], x[:,\u001b[39m1\u001b[39m], lw\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, s\u001b[39m=\u001b[39m\u001b[39m40\u001b[39m,\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/omerkerem.adali/Desktop/tubitak/Triplet-net-keras/Triplet%20NN%20Test%20on%20MNIST.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                 c\u001b[39m=\u001b[39mpalette[labels\u001b[39m.\u001b[39;49mastype(\u001b[39mint\u001b[39;49m)])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/omerkerem.adali/Desktop/tubitak/Triplet-net-keras/Triplet%20NN%20Test%20on%20MNIST.ipynb#X13sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m plt\u001b[39m.\u001b[39mxlim(\u001b[39m-\u001b[39m\u001b[39m25\u001b[39m, \u001b[39m25\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/omerkerem.adali/Desktop/tubitak/Triplet-net-keras/Triplet%20NN%20Test%20on%20MNIST.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m plt\u001b[39m.\u001b[39mylim(\u001b[39m-\u001b[39m\u001b[39m25\u001b[39m, \u001b[39m25\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 10 is out of bounds for axis 0 with size 10"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tsne = TSNE()\n",
    "train_tsne_embeds = tsne.fit_transform(x_train[:512])\n",
    "scatter(train_tsne_embeds, y_train[:512], \"Samples from Training Data\")\n",
    "\n",
    "eval_tsne_embeds = tsne.fit_transform(x_test[:512])\n",
    "scatter(eval_tsne_embeds, y_test[:512], \"Samples from Validation Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Classifier_input = Input((768,))\n",
    "Classifier_output = Dense(10, activation='softmax')(Classifier_input)\n",
    "Classifier_model = Model(Classifier_input, Classifier_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_onehot = le.fit_transform(y_train)\n",
    "y_test_onehot = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Classifier_model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 768)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3/3 [==============================] - 1s 77ms/step - loss: 2.3876 - accuracy: 0.1375 - val_loss: 2.4313 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 2.1830 - accuracy: 0.2125 - val_loss: 2.4130 - val_accuracy: 0.1500\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 2.0952 - accuracy: 0.2875 - val_loss: 2.3458 - val_accuracy: 0.1000\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 2.0058 - accuracy: 0.3125 - val_loss: 2.2864 - val_accuracy: 0.2000\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 1.8986 - accuracy: 0.4125 - val_loss: 2.2429 - val_accuracy: 0.2000\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 1.7967 - accuracy: 0.5250 - val_loss: 2.1943 - val_accuracy: 0.2500\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 1.7193 - accuracy: 0.6500 - val_loss: 2.1629 - val_accuracy: 0.2500\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 1.6601 - accuracy: 0.6375 - val_loss: 2.1627 - val_accuracy: 0.2000\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 1.5818 - accuracy: 0.6375 - val_loss: 2.1329 - val_accuracy: 0.2000\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 1.5146 - accuracy: 0.6375 - val_loss: 2.1108 - val_accuracy: 0.2000\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 1.4521 - accuracy: 0.6500 - val_loss: 2.0729 - val_accuracy: 0.2500\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 1.3940 - accuracy: 0.7000 - val_loss: 2.0393 - val_accuracy: 0.3000\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.3469 - accuracy: 0.7000 - val_loss: 2.0198 - val_accuracy: 0.3000\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 1.2978 - accuracy: 0.7375 - val_loss: 1.9726 - val_accuracy: 0.3000\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 1.2512 - accuracy: 0.7625 - val_loss: 1.9334 - val_accuracy: 0.3500\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 1.2060 - accuracy: 0.7875 - val_loss: 1.9033 - val_accuracy: 0.3500\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 1.1639 - accuracy: 0.7875 - val_loss: 1.8910 - val_accuracy: 0.4000\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.1230 - accuracy: 0.8125 - val_loss: 1.8816 - val_accuracy: 0.3500\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.0916 - accuracy: 0.8000 - val_loss: 1.8692 - val_accuracy: 0.3500\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.0568 - accuracy: 0.8375 - val_loss: 1.8473 - val_accuracy: 0.3500\n",
      "Epoch 21/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.0205 - accuracy: 0.8375 - val_loss: 1.8224 - val_accuracy: 0.3500\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.9853 - accuracy: 0.8500 - val_loss: 1.8076 - val_accuracy: 0.3500\n",
      "Epoch 23/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.9553 - accuracy: 0.8625 - val_loss: 1.7952 - val_accuracy: 0.3500\n",
      "Epoch 24/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.9273 - accuracy: 0.8500 - val_loss: 1.7846 - val_accuracy: 0.3500\n",
      "Epoch 25/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.9005 - accuracy: 0.8500 - val_loss: 1.7752 - val_accuracy: 0.3500\n",
      "Epoch 26/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.8770 - accuracy: 0.8625 - val_loss: 1.7547 - val_accuracy: 0.4000\n",
      "Epoch 27/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.8478 - accuracy: 0.8625 - val_loss: 1.7421 - val_accuracy: 0.4000\n",
      "Epoch 28/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.8240 - accuracy: 0.8625 - val_loss: 1.7287 - val_accuracy: 0.4000\n",
      "Epoch 29/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.8023 - accuracy: 0.8875 - val_loss: 1.7386 - val_accuracy: 0.4500\n",
      "Epoch 30/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.7841 - accuracy: 0.8750 - val_loss: 1.7534 - val_accuracy: 0.3500\n",
      "Epoch 31/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.7615 - accuracy: 0.8750 - val_loss: 1.7478 - val_accuracy: 0.3500\n",
      "Epoch 32/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.7384 - accuracy: 0.8875 - val_loss: 1.7250 - val_accuracy: 0.3500\n",
      "Epoch 33/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.7201 - accuracy: 0.8875 - val_loss: 1.7023 - val_accuracy: 0.3500\n",
      "Epoch 34/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.7002 - accuracy: 0.9000 - val_loss: 1.6712 - val_accuracy: 0.3500\n",
      "Epoch 35/50\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6814 - accuracy: 0.9000 - val_loss: 1.6422 - val_accuracy: 0.4500\n",
      "Epoch 36/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.6648 - accuracy: 0.9250 - val_loss: 1.6171 - val_accuracy: 0.4500\n",
      "Epoch 37/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.6491 - accuracy: 0.9250 - val_loss: 1.6202 - val_accuracy: 0.4500\n",
      "Epoch 38/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.6330 - accuracy: 0.9250 - val_loss: 1.6288 - val_accuracy: 0.4500\n",
      "Epoch 39/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.6165 - accuracy: 0.9375 - val_loss: 1.6406 - val_accuracy: 0.4500\n",
      "Epoch 40/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.6029 - accuracy: 0.9375 - val_loss: 1.6555 - val_accuracy: 0.4500\n",
      "Epoch 41/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.5886 - accuracy: 0.9625 - val_loss: 1.6463 - val_accuracy: 0.4500\n",
      "Epoch 42/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.5726 - accuracy: 0.9625 - val_loss: 1.6315 - val_accuracy: 0.4500\n",
      "Epoch 43/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.5573 - accuracy: 0.9500 - val_loss: 1.6171 - val_accuracy: 0.4000\n",
      "Epoch 44/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.5463 - accuracy: 0.9500 - val_loss: 1.5883 - val_accuracy: 0.4000\n",
      "Epoch 45/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.5353 - accuracy: 0.9375 - val_loss: 1.5825 - val_accuracy: 0.4000\n",
      "Epoch 46/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.5239 - accuracy: 0.9500 - val_loss: 1.5882 - val_accuracy: 0.4000\n",
      "Epoch 47/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.5110 - accuracy: 0.9500 - val_loss: 1.5970 - val_accuracy: 0.4000\n",
      "Epoch 48/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.4978 - accuracy: 0.9625 - val_loss: 1.5974 - val_accuracy: 0.4500\n",
      "Epoch 49/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4851 - accuracy: 0.9750 - val_loss: 1.5950 - val_accuracy: 0.4500\n",
      "Epoch 50/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4758 - accuracy: 0.9750 - val_loss: 1.5980 - val_accuracy: 0.4500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27650f29a80>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Classifier_model.fit(x_train_flat,y_train_onehot, validation_data=(x_test_flat,y_test_onehot),epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_triplet(x,y,testsize=0.3,ap_pairs=10,an_pairs=10):\n",
    "    data_xy = tuple([x,y])\n",
    "\n",
    "    trainsize = 1-testsize\n",
    "\n",
    "    triplet_train_pairs = []\n",
    "    triplet_test_pairs = []\n",
    "    for data_class in sorted(set(data_xy[1])):\n",
    "\n",
    "        same_class_idx = np.where((data_xy[1] == data_class))[0]\n",
    "        diff_class_idx = np.where(data_xy[1] != data_class)[0]\n",
    "        A_P_pairs = random.sample(list(permutations(same_class_idx,2)),k=ap_pairs) #Generating Anchor-Positive pairs\n",
    "        Neg_idx = random.sample(list(diff_class_idx),k=an_pairs)\n",
    "        \n",
    "\n",
    "        #train\n",
    "        A_P_len = len(A_P_pairs)\n",
    "        Neg_len = len(Neg_idx)\n",
    "        for ap in A_P_pairs[:int(A_P_len*trainsize)]:\n",
    "            Anchor = data_xy[0][ap[0]]\n",
    "            Positive = data_xy[0][ap[1]]\n",
    "            for n in Neg_idx:\n",
    "                Negative = data_xy[0][n]\n",
    "                triplet_train_pairs.append([Anchor,Positive,Negative])               \n",
    "        #test\n",
    "        for ap in A_P_pairs[int(A_P_len*trainsize):]:\n",
    "            Anchor = data_xy[0][ap[0]]\n",
    "            Positive = data_xy[0][ap[1]]\n",
    "            for n in Neg_idx:\n",
    "                Negative = data_xy[0][n]\n",
    "                triplet_test_pairs.append([Anchor,Positive,Negative])    \n",
    "                \n",
    "    return np.array(triplet_train_pairs), np.array(triplet_test_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7200, 3, 768)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test = generate_triplet(x_train_flat,y_train, ap_pairs=30, an_pairs=30,testsize=0.2)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triplet NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(y_true, y_pred, alpha = 0.4):\n",
    "    \"\"\"\n",
    "    Implementation of the triplet loss function\n",
    "    Arguments:\n",
    "    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n",
    "    y_pred -- python list containing three objects:\n",
    "            anchor -- the encodings for the anchor data\n",
    "            positive -- the encodings for the positive data (similar to anchor)\n",
    "            negative -- the encodings for the negative data (different from anchor)\n",
    "    Returns:\n",
    "    loss -- real number, value of the loss\n",
    "    \"\"\"\n",
    "    print('y_pred.shape = ',y_pred)\n",
    "    \n",
    "    total_lenght = y_pred.shape.as_list()[-1]\n",
    "#     print('total_lenght=',  total_lenght)\n",
    "#     total_lenght =12\n",
    "    \n",
    "    anchor = y_pred[:,0:int(total_lenght*1/3)]\n",
    "    positive = y_pred[:,int(total_lenght*1/3):int(total_lenght*2/3)]\n",
    "    negative = y_pred[:,int(total_lenght*2/3):int(total_lenght*3/3)]\n",
    "\n",
    "    # distance between the anchor and the positive\n",
    "    pos_dist = K.sum(K.square(anchor-positive),axis=1)\n",
    "\n",
    "    # distance between the anchor and the negative\n",
    "    neg_dist = K.sum(K.square(anchor-negative),axis=1)\n",
    "\n",
    "    # compute loss\n",
    "    basic_loss = pos_dist-neg_dist+alpha\n",
    "    loss = K.maximum(basic_loss,0.0)\n",
    " \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base_network():\n",
    "    \"\"\"\n",
    "    Base network to be shared.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(768,1)))\n",
    "    # model.add(Conv2D(128,(7,7),padding='same',input_shape=(in_dims[0],in_dims[1],in_dims[2],),activation='relu',name='conv1'))\n",
    "    # model.add(MaxPooling2D((2,2),(2,2),padding='same',name='pool1'))\n",
    "    # model.add(Conv2D(256,(5,5),padding='same',activation='relu',name='conv2'))\n",
    "    # model.add(MaxPooling2D((2,2),(2,2),padding='same',name='pool2'))\n",
    "    # model.add(Flatten(name='flatten'))\n",
    "    model.add(Dense(768,name='embeddings'))\n",
    "    # model.add(Dense(600))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "source": [
    "adam_optim = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_input = Input((768,1), name='anchor_input')\n",
    "positive_input = Input((768,1 ), name='positive_input')\n",
    "negative_input = Input((768,1 ), name='negative_input')\n",
    "\n",
    "# Shared embedding layer for positive and negative items\n",
    "Shared_DNN = create_base_network()\n",
    "\n",
    "\n",
    "encoded_anchor = Shared_DNN(anchor_input)\n",
    "encoded_positive = Shared_DNN(positive_input)\n",
    "encoded_negative = Shared_DNN(negative_input)\n",
    "\n",
    "\n",
    "merged_vector = concatenate([encoded_anchor, encoded_positive, encoded_negative], axis=-1, name='merged_layer')\n",
    "\n",
    "model = Model(inputs=[anchor_input,positive_input, negative_input], outputs=merged_vector)\n",
    "model.compile(loss=triplet_loss, optimizer=adam_optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " anchor_input (InputLayer)      [(None, 768, 1)]     0           []                               \n",
      "                                                                                                  \n",
      " positive_input (InputLayer)    [(None, 768, 1)]     0           []                               \n",
      "                                                                                                  \n",
      " negative_input (InputLayer)    [(None, 768, 1)]     0           []                               \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, 768)          590592      ['anchor_input[0][0]',           \n",
      "                                                                  'positive_input[0][0]',         \n",
      "                                                                  'negative_input[0][0]']         \n",
      "                                                                                                  \n",
      " merged_layer (Concatenate)     (None, 2304)         0           ['sequential[0][0]',             \n",
      "                                                                  'sequential[1][0]',             \n",
      "                                                                  'sequential[2][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 590,592\n",
      "Trainable params: 590,592\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7200, 3, 768)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "y_pred.shape =  Tensor(\"model_1/merged_layer/concat:0\", shape=(None, 2304), dtype=float32)\n",
      "y_pred.shape =  Tensor(\"model_1/merged_layer/concat:0\", shape=(None, 2304), dtype=float32)\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 2.5207y_pred.shape =  Tensor(\"model_1/merged_layer/concat:0\", shape=(None, 2304), dtype=float32)\n",
      "15/15 [==============================] - 1s 54ms/step - loss: 2.5095 - val_loss: 1.0319\n",
      "Epoch 2/500\n",
      "15/15 [==============================] - 1s 42ms/step - loss: 0.1578 - val_loss: 0.7343\n",
      "Epoch 3/500\n",
      "15/15 [==============================] - 1s 42ms/step - loss: 0.0749 - val_loss: 0.6154\n",
      "Epoch 4/500\n",
      "15/15 [==============================] - 1s 42ms/step - loss: 0.0525 - val_loss: 0.6452\n",
      "Epoch 5/500\n",
      "15/15 [==============================] - 1s 39ms/step - loss: 0.0603 - val_loss: 0.7221\n",
      "Epoch 6/500\n",
      "15/15 [==============================] - 1s 43ms/step - loss: 0.0508 - val_loss: 0.6975\n",
      "Epoch 7/500\n",
      "15/15 [==============================] - 1s 41ms/step - loss: 0.0354 - val_loss: 0.7264\n",
      "Epoch 8/500\n",
      "15/15 [==============================] - 1s 40ms/step - loss: 0.0281 - val_loss: 0.7627\n",
      "Epoch 9/500\n",
      "15/15 [==============================] - 1s 45ms/step - loss: 0.0241 - val_loss: 0.6553\n",
      "Epoch 10/500\n",
      "15/15 [==============================] - 1s 45ms/step - loss: 0.0216 - val_loss: 0.7044\n",
      "Epoch 11/500\n",
      "15/15 [==============================] - 1s 43ms/step - loss: 0.0397 - val_loss: 0.8051\n",
      "Epoch 12/500\n",
      "15/15 [==============================] - 1s 39ms/step - loss: 0.0363 - val_loss: 0.7536\n",
      "Epoch 13/500\n",
      "15/15 [==============================] - 1s 39ms/step - loss: 0.0256 - val_loss: 0.7109\n",
      "Epoch 14/500\n",
      "15/15 [==============================] - 1s 45ms/step - loss: 0.1095 - val_loss: 0.6346\n",
      "Epoch 15/500\n",
      "15/15 [==============================] - 1s 56ms/step - loss: 0.0443 - val_loss: 0.6186\n",
      "Epoch 16/500\n",
      "15/15 [==============================] - 1s 52ms/step - loss: 0.0242 - val_loss: 0.6150\n",
      "Epoch 17/500\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 0.0214 - val_loss: 0.5997\n",
      "Epoch 18/500\n",
      "15/15 [==============================] - 1s 58ms/step - loss: 0.0757 - val_loss: 0.4949\n",
      "Epoch 19/500\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 0.0381 - val_loss: 0.5017\n",
      "Epoch 20/500\n",
      "15/15 [==============================] - 1s 50ms/step - loss: 0.0233 - val_loss: 0.4264\n",
      "Epoch 21/500\n",
      "15/15 [==============================] - 1s 51ms/step - loss: 0.0181 - val_loss: 0.4190\n",
      "Epoch 22/500\n",
      "15/15 [==============================] - 1s 49ms/step - loss: 0.0135 - val_loss: 0.4516\n",
      "Epoch 23/500\n",
      "15/15 [==============================] - 1s 49ms/step - loss: 0.0127 - val_loss: 0.4498\n",
      "Epoch 24/500\n",
      "15/15 [==============================] - 1s 52ms/step - loss: 0.0218 - val_loss: 0.5438\n",
      "Epoch 25/500\n",
      "15/15 [==============================] - 1s 52ms/step - loss: 0.0155 - val_loss: 0.5538\n",
      "Epoch 26/500\n",
      "15/15 [==============================] - 1s 52ms/step - loss: 0.0129 - val_loss: 0.5498\n",
      "Epoch 27/500\n",
      "15/15 [==============================] - 1s 52ms/step - loss: 0.0093 - val_loss: 0.5486\n",
      "Epoch 28/500\n",
      "15/15 [==============================] - 1s 54ms/step - loss: 0.0088 - val_loss: 0.5706\n",
      "Epoch 29/500\n",
      "15/15 [==============================] - 1s 50ms/step - loss: 0.0072 - val_loss: 0.5653\n",
      "Epoch 30/500\n",
      "15/15 [==============================] - 1s 49ms/step - loss: 0.0079 - val_loss: 0.5861\n",
      "Epoch 31/500\n",
      "15/15 [==============================] - 1s 49ms/step - loss: 0.0148 - val_loss: 0.6157\n",
      "Epoch 32/500\n",
      "15/15 [==============================] - 1s 46ms/step - loss: 0.0122 - val_loss: 0.5348\n",
      "Epoch 33/500\n",
      "15/15 [==============================] - 1s 58ms/step - loss: 0.0124 - val_loss: 0.5119\n",
      "Epoch 34/500\n",
      "15/15 [==============================] - 1s 50ms/step - loss: 0.0141 - val_loss: 0.5236\n",
      "Epoch 35/500\n",
      "15/15 [==============================] - 1s 54ms/step - loss: 0.0082 - val_loss: 0.5127\n",
      "Epoch 36/500\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 0.0066 - val_loss: 0.5300\n",
      "Epoch 37/500\n",
      "15/15 [==============================] - 1s 50ms/step - loss: 0.0070 - val_loss: 0.5478\n",
      "Epoch 38/500\n",
      "15/15 [==============================] - 1s 50ms/step - loss: 0.0060 - val_loss: 0.5544\n",
      "Epoch 39/500\n",
      "15/15 [==============================] - 1s 48ms/step - loss: 0.0072 - val_loss: 0.5671\n",
      "Epoch 40/500\n",
      "15/15 [==============================] - 1s 49ms/step - loss: 0.0056 - val_loss: 0.5748\n",
      "Epoch 41/500\n",
      "15/15 [==============================] - 1s 53ms/step - loss: 0.0070 - val_loss: 0.5608\n",
      "Epoch 42/500\n",
      "15/15 [==============================] - 1s 45ms/step - loss: 0.0073 - val_loss: 0.5830\n",
      "Epoch 43/500\n",
      "15/15 [==============================] - 1s 42ms/step - loss: 0.0077 - val_loss: 0.5704\n",
      "Epoch 44/500\n",
      "15/15 [==============================] - 1s 40ms/step - loss: 0.0049 - val_loss: 0.5850\n",
      "Epoch 45/500\n",
      "15/15 [==============================] - 1s 40ms/step - loss: 0.0052 - val_loss: 0.5883\n",
      "Epoch 46/500\n",
      "15/15 [==============================] - 1s 42ms/step - loss: 0.0076 - val_loss: 0.5689\n",
      "Epoch 47/500\n",
      "15/15 [==============================] - 1s 39ms/step - loss: 0.0070 - val_loss: 0.6230\n",
      "Epoch 48/500\n",
      "15/15 [==============================] - 1s 43ms/step - loss: 0.0478 - val_loss: 0.5564\n",
      "Epoch 49/500\n",
      "15/15 [==============================] - 1s 41ms/step - loss: 0.0179 - val_loss: 0.4418\n",
      "Epoch 50/500\n",
      "15/15 [==============================] - 1s 41ms/step - loss: 0.0182 - val_loss: 0.3863\n",
      "Epoch 51/500\n",
      "15/15 [==============================] - 1s 44ms/step - loss: 0.0112 - val_loss: 0.3616\n",
      "Epoch 52/500\n",
      "15/15 [==============================] - 1s 40ms/step - loss: 0.0117 - val_loss: 0.3804\n",
      "Epoch 53/500\n",
      "15/15 [==============================] - 1s 43ms/step - loss: 0.0087 - val_loss: 0.4142\n",
      "Epoch 54/500\n",
      "15/15 [==============================] - 1s 47ms/step - loss: 0.0106 - val_loss: 0.3990\n",
      "Epoch 55/500\n",
      "15/15 [==============================] - 1s 42ms/step - loss: 0.0087 - val_loss: 0.4006\n",
      "Epoch 56/500\n",
      "15/15 [==============================] - 1s 40ms/step - loss: 0.0085 - val_loss: 0.5419\n",
      "Epoch 57/500\n",
      "15/15 [==============================] - 1s 43ms/step - loss: 0.0080 - val_loss: 0.4305\n",
      "Epoch 58/500\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 0.0096 - val_loss: 0.4078\n",
      "Epoch 59/500\n",
      "15/15 [==============================] - 1s 41ms/step - loss: 0.0604 - val_loss: 0.2927\n",
      "Epoch 60/500\n",
      "15/15 [==============================] - 1s 40ms/step - loss: 0.0412 - val_loss: 0.3942\n",
      "Epoch 61/500\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 0.0240 - val_loss: 0.3364\n",
      "Epoch 62/500\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0224 - val_loss: 0.3845\n",
      "Epoch 63/500\n",
      "15/15 [==============================] - 1s 40ms/step - loss: 0.0195 - val_loss: 0.2781\n",
      "Epoch 64/500\n",
      "15/15 [==============================] - 1s 56ms/step - loss: 0.0153 - val_loss: 0.2736\n",
      "Epoch 65/500\n",
      "15/15 [==============================] - 1s 45ms/step - loss: 0.0111 - val_loss: 0.2245\n",
      "Epoch 66/500\n",
      "15/15 [==============================] - 1s 45ms/step - loss: 0.0147 - val_loss: 0.2362\n",
      "Epoch 67/500\n",
      "15/15 [==============================] - 1s 41ms/step - loss: 0.0541 - val_loss: 0.2152\n",
      "Epoch 68/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0273 - val_loss: 0.2645\n",
      "Epoch 69/500\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0810 - val_loss: 0.2160\n",
      "Epoch 70/500\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0340 - val_loss: 0.3756\n",
      "Epoch 71/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0325 - val_loss: 0.3477\n",
      "Epoch 72/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0222 - val_loss: 0.3988\n",
      "Epoch 73/500\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0183 - val_loss: 0.3447\n",
      "Epoch 74/500\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0125 - val_loss: 0.3622\n",
      "Epoch 75/500\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0083 - val_loss: 0.3861\n",
      "Epoch 76/500\n",
      "15/15 [==============================] - 1s 41ms/step - loss: 0.0089 - val_loss: 0.3968\n",
      "Epoch 77/500\n",
      "15/15 [==============================] - 1s 46ms/step - loss: 0.0081 - val_loss: 0.4055\n",
      "Epoch 78/500\n",
      "15/15 [==============================] - 1s 51ms/step - loss: 0.0072 - val_loss: 0.4105\n",
      "Epoch 79/500\n",
      "15/15 [==============================] - 1s 48ms/step - loss: 0.0078 - val_loss: 0.4322\n",
      "Epoch 80/500\n",
      "15/15 [==============================] - 1s 50ms/step - loss: 0.0132 - val_loss: 0.4302\n",
      "Epoch 81/500\n",
      "15/15 [==============================] - 1s 42ms/step - loss: 0.0088 - val_loss: 0.4432\n",
      "Epoch 82/500\n",
      "15/15 [==============================] - 1s 60ms/step - loss: 0.0073 - val_loss: 0.4621\n",
      "Epoch 83/500\n",
      "15/15 [==============================] - 1s 56ms/step - loss: 0.0075 - val_loss: 0.4658\n",
      "Epoch 84/500\n",
      "15/15 [==============================] - 1s 55ms/step - loss: 0.0061 - val_loss: 0.4718\n",
      "Epoch 85/500\n",
      "15/15 [==============================] - 1s 45ms/step - loss: 0.0040 - val_loss: 0.4735\n",
      "Epoch 86/500\n",
      "15/15 [==============================] - 1s 43ms/step - loss: 0.0091 - val_loss: 0.4718\n",
      "Epoch 87/500\n",
      "15/15 [==============================] - 1s 45ms/step - loss: 0.0047 - val_loss: 0.4794\n",
      "Epoch 88/500\n",
      "15/15 [==============================] - 1s 46ms/step - loss: 0.0048 - val_loss: 0.4819\n",
      "Epoch 89/500\n",
      "15/15 [==============================] - 1s 53ms/step - loss: 0.0033 - val_loss: 0.4889\n",
      "Epoch 90/500\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 0.0051 - val_loss: 0.5026\n",
      "Epoch 91/500\n",
      "15/15 [==============================] - 1s 59ms/step - loss: 0.0109 - val_loss: 0.5116\n",
      "Epoch 92/500\n",
      "15/15 [==============================] - 1s 54ms/step - loss: 0.0069 - val_loss: 0.4966\n",
      "Epoch 93/500\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.0076 - val_loss: 0.4763\n",
      "Epoch 94/500\n",
      "15/15 [==============================] - 1s 60ms/step - loss: 0.0060 - val_loss: 0.4732\n",
      "Epoch 95/500\n",
      "15/15 [==============================] - 1s 53ms/step - loss: 0.0054 - val_loss: 0.4981\n",
      "Epoch 96/500\n",
      "15/15 [==============================] - 1s 48ms/step - loss: 0.0038 - val_loss: 0.4832\n",
      "Epoch 97/500\n",
      "15/15 [==============================] - 1s 49ms/step - loss: 0.0046 - val_loss: 0.4988\n",
      "Epoch 98/500\n",
      "15/15 [==============================] - 1s 45ms/step - loss: 0.0024 - val_loss: 0.5163\n",
      "Epoch 99/500\n",
      "15/15 [==============================] - 1s 52ms/step - loss: 0.0046 - val_loss: 0.5194\n",
      "Epoch 100/500\n",
      "15/15 [==============================] - 1s 56ms/step - loss: 0.0060 - val_loss: 0.4787\n",
      "Epoch 101/500\n",
      "15/15 [==============================] - 1s 50ms/step - loss: 0.0129 - val_loss: 0.4425\n",
      "Epoch 102/500\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 0.0062 - val_loss: 0.4190\n",
      "Epoch 103/500\n",
      "15/15 [==============================] - 1s 51ms/step - loss: 0.0049 - val_loss: 0.3927\n",
      "Epoch 104/500\n",
      "15/15 [==============================] - 1s 54ms/step - loss: 0.0048 - val_loss: 0.4098\n",
      "Epoch 105/500\n",
      "15/15 [==============================] - 1s 60ms/step - loss: 0.0035 - val_loss: 0.4228\n",
      "Epoch 106/500\n",
      "15/15 [==============================] - 1s 57ms/step - loss: 0.0027 - val_loss: 0.4362\n",
      "Epoch 107/500\n",
      "15/15 [==============================] - 1s 59ms/step - loss: 0.0027 - val_loss: 0.4379\n",
      "Epoch 108/500\n",
      "15/15 [==============================] - 1s 53ms/step - loss: 0.0027 - val_loss: 0.4431\n",
      "Epoch 109/500\n",
      "15/15 [==============================] - 1s 65ms/step - loss: 0.0025 - val_loss: 0.4471\n",
      "Epoch 110/500\n",
      "15/15 [==============================] - 1s 52ms/step - loss: 0.0024 - val_loss: 0.4454\n",
      "Epoch 111/500\n",
      "15/15 [==============================] - 1s 49ms/step - loss: 0.0026 - val_loss: 0.4544\n",
      "Epoch 112/500\n",
      "15/15 [==============================] - 1s 55ms/step - loss: 0.0028 - val_loss: 0.4415\n",
      "Epoch 113/500\n",
      "15/15 [==============================] - 1s 47ms/step - loss: 0.0059 - val_loss: 0.4806\n",
      "Epoch 114/500\n",
      "15/15 [==============================] - 1s 49ms/step - loss: 0.0065 - val_loss: 0.4445\n",
      "Epoch 115/500\n",
      "15/15 [==============================] - 1s 50ms/step - loss: 0.0045 - val_loss: 0.4424\n",
      "Epoch 116/500\n",
      "15/15 [==============================] - 1s 52ms/step - loss: 0.0033 - val_loss: 0.4285\n",
      "Epoch 117/500\n",
      "15/15 [==============================] - 1s 57ms/step - loss: 0.0030 - val_loss: 0.4258\n",
      "Epoch 118/500\n",
      "15/15 [==============================] - 2s 120ms/step - loss: 0.0030 - val_loss: 0.4325\n",
      "Epoch 119/500\n",
      "15/15 [==============================] - 1s 95ms/step - loss: 0.0031 - val_loss: 0.4368\n",
      "Epoch 120/500\n",
      "15/15 [==============================] - 2s 134ms/step - loss: 0.0030 - val_loss: 0.4340\n",
      "Epoch 121/500\n",
      "15/15 [==============================] - 2s 143ms/step - loss: 0.0025 - val_loss: 0.4363\n",
      "Epoch 122/500\n",
      "15/15 [==============================] - 1s 72ms/step - loss: 0.0125 - val_loss: 0.4618\n",
      "Epoch 123/500\n",
      "15/15 [==============================] - 1s 65ms/step - loss: 0.0243 - val_loss: 0.3967\n",
      "Epoch 124/500\n",
      "15/15 [==============================] - 1s 60ms/step - loss: 0.0178 - val_loss: 0.3162\n",
      "Epoch 125/500\n",
      "15/15 [==============================] - 1s 57ms/step - loss: 0.0352 - val_loss: 0.1648\n",
      "Epoch 126/500\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 0.0420 - val_loss: 0.2204\n",
      "Epoch 127/500\n",
      "15/15 [==============================] - 1s 54ms/step - loss: 0.0466 - val_loss: 0.3022\n",
      "Epoch 128/500\n",
      "15/15 [==============================] - 1s 52ms/step - loss: 0.0654 - val_loss: 0.5300\n",
      "Epoch 129/500\n",
      "15/15 [==============================] - 1s 49ms/step - loss: 0.2127 - val_loss: 0.3769\n",
      "Epoch 130/500\n",
      "15/15 [==============================] - 1s 58ms/step - loss: 0.1562 - val_loss: 0.6887\n",
      "Epoch 131/500\n",
      "15/15 [==============================] - 1s 82ms/step - loss: 0.1304 - val_loss: 0.2494\n",
      "Epoch 132/500\n",
      "15/15 [==============================] - 1s 79ms/step - loss: 0.0999 - val_loss: 0.6626\n",
      "Epoch 133/500\n",
      "15/15 [==============================] - 2s 114ms/step - loss: 0.0723 - val_loss: 0.2827\n",
      "Epoch 134/500\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.0665 - val_loss: 0.8082\n",
      "Epoch 135/500\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.0819 - val_loss: 0.4738\n",
      "Epoch 136/500\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.0618 - val_loss: 0.6359\n",
      "Epoch 137/500\n",
      "15/15 [==============================] - 2s 124ms/step - loss: 0.0948 - val_loss: 0.5486\n",
      "Epoch 138/500\n",
      "15/15 [==============================] - 2s 99ms/step - loss: 0.0904 - val_loss: 0.4932\n",
      "Epoch 139/500\n",
      "15/15 [==============================] - 1s 52ms/step - loss: 0.0548 - val_loss: 0.4435\n",
      "Epoch 140/500\n",
      "15/15 [==============================] - 1s 46ms/step - loss: 0.0395 - val_loss: 0.4476\n",
      "Epoch 141/500\n",
      "15/15 [==============================] - 1s 53ms/step - loss: 0.0314 - val_loss: 0.3625\n",
      "Epoch 142/500\n",
      "15/15 [==============================] - 1s 54ms/step - loss: 0.0255 - val_loss: 0.4386\n",
      "Epoch 143/500\n",
      "15/15 [==============================] - 1s 40ms/step - loss: 0.0155 - val_loss: 0.4079\n",
      "Epoch 144/500\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 0.0185 - val_loss: 0.4100\n",
      "Epoch 145/500\n",
      "15/15 [==============================] - 1s 39ms/step - loss: 0.0164 - val_loss: 0.4090\n",
      "Epoch 146/500\n",
      "15/15 [==============================] - 1s 39ms/step - loss: 0.0127 - val_loss: 0.4381\n",
      "Epoch 147/500\n",
      "15/15 [==============================] - 1s 39ms/step - loss: 0.0109 - val_loss: 0.4071\n",
      "Epoch 148/500\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0117 - val_loss: 0.4377\n",
      "Epoch 149/500\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 0.0097 - val_loss: 0.4207\n",
      "Epoch 150/500\n",
      "15/15 [==============================] - 1s 40ms/step - loss: 0.0134 - val_loss: 0.4673\n",
      "Epoch 151/500\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0082 - val_loss: 0.4449\n",
      "Epoch 152/500\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0093 - val_loss: 0.4449\n",
      "Epoch 153/500\n",
      "15/15 [==============================] - 1s 40ms/step - loss: 0.0073 - val_loss: 0.4414\n",
      "Epoch 154/500\n",
      "15/15 [==============================] - 1s 40ms/step - loss: 0.0068 - val_loss: 0.4644\n",
      "Epoch 155/500\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 0.0086 - val_loss: 0.4613\n",
      "Epoch 156/500\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0078 - val_loss: 0.4348\n",
      "Epoch 157/500\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0088 - val_loss: 0.4718\n",
      "Epoch 158/500\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0089 - val_loss: 0.4324\n",
      "Epoch 159/500\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0379 - val_loss: 0.4920\n",
      "Epoch 160/500\n",
      "15/15 [==============================] - 1s 39ms/step - loss: 0.0423 - val_loss: 0.3941\n",
      "Epoch 161/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0247 - val_loss: 0.4106\n",
      "Epoch 162/500\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0195 - val_loss: 0.3781\n",
      "Epoch 163/500\n",
      "15/15 [==============================] - 1s 46ms/step - loss: 0.0175 - val_loss: 0.3666\n",
      "Epoch 164/500\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0114 - val_loss: 0.3841\n",
      "Epoch 165/500\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 0.0104 - val_loss: 0.4017\n",
      "Epoch 166/500\n",
      "15/15 [==============================] - 1s 40ms/step - loss: 0.0467 - val_loss: 0.3846\n",
      "Epoch 167/500\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 0.0313 - val_loss: 0.3181\n",
      "Epoch 168/500\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0176 - val_loss: 0.2931\n",
      "Epoch 169/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0145 - val_loss: 0.3157\n",
      "Epoch 170/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0181 - val_loss: 0.3826\n",
      "Epoch 171/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0275 - val_loss: 0.3198\n",
      "Epoch 172/500\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 0.0250 - val_loss: 0.3786\n",
      "Epoch 173/500\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0197 - val_loss: 0.3721\n",
      "Epoch 174/500\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0196 - val_loss: 0.3675\n",
      "Epoch 175/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0098 - val_loss: 0.3791\n",
      "Epoch 176/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0072 - val_loss: 0.3909\n",
      "Epoch 177/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0061 - val_loss: 0.3980\n",
      "Epoch 178/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0050 - val_loss: 0.4049\n",
      "Epoch 179/500\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0046 - val_loss: 0.4127\n",
      "Epoch 180/500\n",
      "15/15 [==============================] - 1s 47ms/step - loss: 0.0063 - val_loss: 0.4101\n",
      "Epoch 181/500\n",
      "15/15 [==============================] - 1s 59ms/step - loss: 0.0056 - val_loss: 0.4216\n",
      "Epoch 182/500\n",
      "15/15 [==============================] - 1s 51ms/step - loss: 0.0063 - val_loss: 0.4004\n",
      "Epoch 183/500\n",
      "15/15 [==============================] - 1s 50ms/step - loss: 0.0113 - val_loss: 0.4115\n",
      "Epoch 184/500\n",
      "15/15 [==============================] - 1s 48ms/step - loss: 0.0166 - val_loss: 0.3904\n",
      "Epoch 185/500\n",
      "15/15 [==============================] - 1s 52ms/step - loss: 0.0086 - val_loss: 0.3993\n",
      "Epoch 186/500\n",
      "15/15 [==============================] - 1s 52ms/step - loss: 0.0083 - val_loss: 0.4124\n",
      "Epoch 187/500\n",
      "15/15 [==============================] - 1s 59ms/step - loss: 0.0100 - val_loss: 0.4315\n",
      "Epoch 188/500\n",
      "15/15 [==============================] - 1s 45ms/step - loss: 0.0112 - val_loss: 0.4344\n",
      "Epoch 189/500\n",
      "15/15 [==============================] - 1s 41ms/step - loss: 0.0143 - val_loss: 0.4449\n",
      "Epoch 190/500\n",
      "15/15 [==============================] - 1s 40ms/step - loss: 0.0193 - val_loss: 0.4258\n",
      "Epoch 191/500\n",
      "15/15 [==============================] - 1s 43ms/step - loss: 0.0081 - val_loss: 0.4208\n",
      "Epoch 192/500\n",
      "15/15 [==============================] - 1s 47ms/step - loss: 0.0084 - val_loss: 0.4222\n",
      "Epoch 193/500\n",
      "15/15 [==============================] - 1s 47ms/step - loss: 0.0061 - val_loss: 0.4300\n",
      "Epoch 194/500\n",
      "15/15 [==============================] - 1s 47ms/step - loss: 0.0042 - val_loss: 0.4369\n",
      "Epoch 195/500\n",
      "15/15 [==============================] - 1s 48ms/step - loss: 0.0037 - val_loss: 0.4421\n",
      "Epoch 196/500\n",
      "15/15 [==============================] - 1s 44ms/step - loss: 0.0040 - val_loss: 0.4503\n",
      "Epoch 197/500\n",
      "15/15 [==============================] - 1s 43ms/step - loss: 0.0043 - val_loss: 0.4479\n",
      "Epoch 198/500\n",
      "15/15 [==============================] - 1s 43ms/step - loss: 0.0036 - val_loss: 0.4509\n",
      "Epoch 199/500\n",
      "15/15 [==============================] - 1s 39ms/step - loss: 0.0037 - val_loss: 0.4578\n",
      "Epoch 200/500\n",
      "15/15 [==============================] - 1s 46ms/step - loss: 0.0031 - val_loss: 0.4595\n",
      "Epoch 201/500\n",
      "15/15 [==============================] - 1s 41ms/step - loss: 0.0030 - val_loss: 0.4585\n",
      "Epoch 202/500\n",
      "15/15 [==============================] - 1s 45ms/step - loss: 0.0034 - val_loss: 0.4650\n",
      "Epoch 203/500\n",
      "15/15 [==============================] - 1s 40ms/step - loss: 0.0040 - val_loss: 0.4623\n",
      "Epoch 204/500\n",
      "15/15 [==============================] - 1s 42ms/step - loss: 0.0041 - val_loss: 0.4537\n",
      "Epoch 205/500\n",
      "15/15 [==============================] - 1s 39ms/step - loss: 0.0036 - val_loss: 0.4556\n",
      "Epoch 206/500\n",
      "15/15 [==============================] - 1s 40ms/step - loss: 0.0031 - val_loss: 0.4606\n",
      "Epoch 207/500\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 0.0040 - val_loss: 0.4602\n",
      "Epoch 208/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0031 - val_loss: 0.4606\n",
      "Epoch 209/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0034 - val_loss: 0.4625\n",
      "Epoch 210/500\n",
      "15/15 [==============================] - 1s 65ms/step - loss: 0.0034 - val_loss: 0.4622\n",
      "Epoch 211/500\n",
      "15/15 [==============================] - 1s 56ms/step - loss: 0.0029 - val_loss: 0.4604\n",
      "Epoch 212/500\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 0.0032 - val_loss: 0.4686\n",
      "Epoch 213/500\n",
      "15/15 [==============================] - 1s 40ms/step - loss: 0.0040 - val_loss: 0.4592\n",
      "Epoch 214/500\n",
      "15/15 [==============================] - 1s 40ms/step - loss: 0.0031 - val_loss: 0.4540\n",
      "Epoch 215/500\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0030 - val_loss: 0.4579\n",
      "Epoch 216/500\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0038 - val_loss: 0.4624\n",
      "Epoch 217/500\n",
      "15/15 [==============================] - 1s 41ms/step - loss: 0.0072 - val_loss: 0.4518\n",
      "Epoch 218/500\n",
      "15/15 [==============================] - 1s 47ms/step - loss: 0.0041 - val_loss: 0.4688\n",
      "Epoch 219/500\n",
      "15/15 [==============================] - 1s 47ms/step - loss: 0.0299 - val_loss: 0.4034\n",
      "Epoch 220/500\n",
      "15/15 [==============================] - 1s 49ms/step - loss: 0.0285 - val_loss: 0.5069\n",
      "Epoch 221/500\n",
      "15/15 [==============================] - 1s 41ms/step - loss: 0.0161 - val_loss: 0.3674\n",
      "Epoch 222/500\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0423 - val_loss: 0.1483\n",
      "Epoch 223/500\n",
      "15/15 [==============================] - 1s 44ms/step - loss: 0.0498 - val_loss: 0.3223\n",
      "Epoch 224/500\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 0.0442 - val_loss: 0.1740\n",
      "Epoch 225/500\n",
      "15/15 [==============================] - 1s 42ms/step - loss: 0.1321 - val_loss: 0.2692\n",
      "Epoch 226/500\n",
      "15/15 [==============================] - 1s 48ms/step - loss: 0.0838 - val_loss: 0.8884\n",
      "Epoch 227/500\n",
      "15/15 [==============================] - 1s 44ms/step - loss: 0.0676 - val_loss: 0.2838\n",
      "Epoch 228/500\n",
      "15/15 [==============================] - 1s 47ms/step - loss: 0.0706 - val_loss: 0.5864\n",
      "Epoch 229/500\n",
      "15/15 [==============================] - 1s 48ms/step - loss: 0.1539 - val_loss: 0.4616\n",
      "Epoch 230/500\n",
      "15/15 [==============================] - 1s 45ms/step - loss: 0.1578 - val_loss: 0.5513\n",
      "Epoch 231/500\n",
      "15/15 [==============================] - 1s 42ms/step - loss: 0.1155 - val_loss: 0.7066\n",
      "Epoch 232/500\n",
      "15/15 [==============================] - 1s 43ms/step - loss: 0.1322 - val_loss: 1.0939\n",
      "Epoch 233/500\n",
      "15/15 [==============================] - 1s 42ms/step - loss: 0.1240 - val_loss: 1.2435\n",
      "Epoch 234/500\n",
      "15/15 [==============================] - 1s 40ms/step - loss: 0.0899 - val_loss: 1.1690\n",
      "Epoch 235/500\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 0.0802 - val_loss: 0.9889\n",
      "Epoch 236/500\n",
      "15/15 [==============================] - 1s 40ms/step - loss: 0.0706 - val_loss: 0.6534\n",
      "Epoch 237/500\n",
      "15/15 [==============================] - 1s 39ms/step - loss: 0.0474 - val_loss: 0.5642\n",
      "Epoch 238/500\n",
      "15/15 [==============================] - 1s 40ms/step - loss: 0.0431 - val_loss: 0.5705\n",
      "Epoch 239/500\n",
      "15/15 [==============================] - 1s 41ms/step - loss: 0.0261 - val_loss: 0.6106\n",
      "Epoch 240/500\n",
      "15/15 [==============================] - 1s 41ms/step - loss: 0.0297 - val_loss: 0.5872\n",
      "Epoch 241/500\n",
      "15/15 [==============================] - 1s 40ms/step - loss: 0.0319 - val_loss: 0.5835\n",
      "Epoch 242/500\n",
      "15/15 [==============================] - 1s 43ms/step - loss: 0.0173 - val_loss: 0.5739\n",
      "Epoch 243/500\n",
      "15/15 [==============================] - 1s 41ms/step - loss: 0.0232 - val_loss: 0.6111\n",
      "Epoch 244/500\n",
      "15/15 [==============================] - 1s 43ms/step - loss: 0.0156 - val_loss: 0.5997\n",
      "Epoch 245/500\n",
      "15/15 [==============================] - 1s 39ms/step - loss: 0.0174 - val_loss: 0.6444\n",
      "Epoch 246/500\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0157 - val_loss: 0.6144\n",
      "Epoch 247/500\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0184 - val_loss: 0.6508\n",
      "Epoch 248/500\n",
      "15/15 [==============================] - 1s 39ms/step - loss: 0.0137 - val_loss: 0.6587\n",
      "Epoch 249/500\n",
      "15/15 [==============================] - 1s 40ms/step - loss: 0.0139 - val_loss: 0.6445\n",
      "Epoch 250/500\n",
      "15/15 [==============================] - 1s 42ms/step - loss: 0.0166 - val_loss: 0.6619\n",
      "Epoch 251/500\n",
      "15/15 [==============================] - 1s 43ms/step - loss: 0.0100 - val_loss: 0.6877\n",
      "Epoch 252/500\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0081 - val_loss: 0.6902\n",
      "Epoch 253/500\n",
      "15/15 [==============================] - 1s 41ms/step - loss: 0.0081 - val_loss: 0.6833\n",
      "Epoch 254/500\n",
      "15/15 [==============================] - 1s 39ms/step - loss: 0.0090 - val_loss: 0.6857\n",
      "Epoch 255/500\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 0.0067 - val_loss: 0.6818\n",
      "Epoch 256/500\n",
      "15/15 [==============================] - 1s 40ms/step - loss: 0.0070 - val_loss: 0.6847\n",
      "Epoch 257/500\n",
      "15/15 [==============================] - 1s 39ms/step - loss: 0.0065 - val_loss: 0.7022\n",
      "Epoch 258/500\n",
      "15/15 [==============================] - 1s 40ms/step - loss: 0.0074 - val_loss: 0.7013\n",
      "Epoch 259/500\n",
      "15/15 [==============================] - 1s 53ms/step - loss: 0.0065 - val_loss: 0.7004\n",
      "Epoch 260/500\n",
      "15/15 [==============================] - 1s 46ms/step - loss: 0.0107 - val_loss: 0.6956\n",
      "Epoch 261/500\n",
      "15/15 [==============================] - 1s 47ms/step - loss: 0.0168 - val_loss: 0.6900\n",
      "Epoch 262/500\n",
      "15/15 [==============================] - 1s 46ms/step - loss: 0.0168 - val_loss: 0.6807\n",
      "Epoch 263/500\n",
      "15/15 [==============================] - 1s 50ms/step - loss: 0.0106 - val_loss: 0.6849\n",
      "Epoch 264/500\n",
      "15/15 [==============================] - 1s 52ms/step - loss: 0.0080 - val_loss: 0.6773\n",
      "Epoch 265/500\n",
      "15/15 [==============================] - 1s 53ms/step - loss: 0.0357 - val_loss: 0.6366\n",
      "Epoch 266/500\n",
      "15/15 [==============================] - 1s 50ms/step - loss: 0.0859 - val_loss: 0.6276\n",
      "Epoch 267/500\n",
      "15/15 [==============================] - 1s 43ms/step - loss: 0.0309 - val_loss: 0.6382\n",
      "Epoch 268/500\n",
      "15/15 [==============================] - 1s 45ms/step - loss: 0.0266 - val_loss: 0.6606\n",
      "Epoch 269/500\n",
      "15/15 [==============================] - 1s 41ms/step - loss: 0.0446 - val_loss: 0.6931\n",
      "Epoch 270/500\n",
      "15/15 [==============================] - 1s 39ms/step - loss: 0.0731 - val_loss: 0.6138\n",
      "Epoch 271/500\n",
      "15/15 [==============================] - 1s 40ms/step - loss: 0.0313 - val_loss: 0.6236\n",
      "Epoch 272/500\n",
      "15/15 [==============================] - 1s 40ms/step - loss: 0.0180 - val_loss: 0.6561\n",
      "Epoch 273/500\n",
      "15/15 [==============================] - 1s 45ms/step - loss: 0.1718 - val_loss: 0.7078\n",
      "Epoch 274/500\n",
      "15/15 [==============================] - 1s 48ms/step - loss: 0.0860 - val_loss: 0.6407\n",
      "Epoch 275/500\n",
      "15/15 [==============================] - 1s 47ms/step - loss: 0.0452 - val_loss: 0.5842\n",
      "Epoch 276/500\n",
      "15/15 [==============================] - 1s 44ms/step - loss: 0.0416 - val_loss: 0.6346\n",
      "Epoch 277/500\n",
      "15/15 [==============================] - 1s 46ms/step - loss: 0.0459 - val_loss: 0.6207\n",
      "Epoch 278/500\n",
      "15/15 [==============================] - 1s 47ms/step - loss: 0.0362 - val_loss: 0.6681\n",
      "Epoch 279/500\n",
      "15/15 [==============================] - 1s 42ms/step - loss: 0.0313 - val_loss: 0.5969\n",
      "Epoch 280/500\n",
      "15/15 [==============================] - 1s 41ms/step - loss: 0.0273 - val_loss: 0.6462\n",
      "Epoch 281/500\n",
      "15/15 [==============================] - 1s 47ms/step - loss: 0.0148 - val_loss: 0.6562\n",
      "Epoch 282/500\n",
      "15/15 [==============================] - 1s 40ms/step - loss: 0.0144 - val_loss: 0.6759\n",
      "Epoch 283/500\n",
      "15/15 [==============================] - 1s 40ms/step - loss: 0.0154 - val_loss: 0.6685\n",
      "Epoch 284/500\n",
      "15/15 [==============================] - 1s 41ms/step - loss: 0.0142 - val_loss: 0.6698\n",
      "Epoch 285/500\n",
      "15/15 [==============================] - 1s 41ms/step - loss: 0.0178 - val_loss: 0.6747\n",
      "Epoch 286/500\n",
      "15/15 [==============================] - 1s 42ms/step - loss: 0.0190 - val_loss: 0.6844\n",
      "Epoch 287/500\n",
      "15/15 [==============================] - 1s 41ms/step - loss: 0.0135 - val_loss: 0.6957\n",
      "Epoch 288/500\n",
      "15/15 [==============================] - 1s 42ms/step - loss: 0.0127 - val_loss: 0.7009\n",
      "Epoch 289/500\n",
      "15/15 [==============================] - 1s 44ms/step - loss: 0.0100 - val_loss: 0.7224\n",
      "Epoch 290/500\n",
      "15/15 [==============================] - 1s 44ms/step - loss: 0.0098 - val_loss: 0.7362\n",
      "Epoch 291/500\n",
      "15/15 [==============================] - 1s 46ms/step - loss: 0.0098 - val_loss: 0.7132\n",
      "Epoch 292/500\n",
      "15/15 [==============================] - 1s 45ms/step - loss: 0.0094 - val_loss: 0.7153\n",
      "Epoch 293/500\n",
      "15/15 [==============================] - 1s 48ms/step - loss: 0.0075 - val_loss: 0.7200\n",
      "Epoch 294/500\n",
      "15/15 [==============================] - 1s 47ms/step - loss: 0.0099 - val_loss: 0.7276\n",
      "Epoch 295/500\n",
      "15/15 [==============================] - 1s 46ms/step - loss: 0.0086 - val_loss: 0.7149\n",
      "Epoch 296/500\n",
      "15/15 [==============================] - 1s 44ms/step - loss: 0.0078 - val_loss: 0.7374\n",
      "Epoch 297/500\n",
      "15/15 [==============================] - 1s 40ms/step - loss: 0.0073 - val_loss: 0.7273\n",
      "Epoch 298/500\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 0.0071 - val_loss: 0.7289\n",
      "Epoch 299/500\n",
      "15/15 [==============================] - 1s 42ms/step - loss: 0.0067 - val_loss: 0.7113\n",
      "Epoch 300/500\n",
      "15/15 [==============================] - 1s 46ms/step - loss: 0.0081 - val_loss: 0.7088\n",
      "Epoch 301/500\n",
      "15/15 [==============================] - 1s 56ms/step - loss: 0.0066 - val_loss: 0.7209\n",
      "Epoch 302/500\n",
      "15/15 [==============================] - 1s 50ms/step - loss: 0.0171 - val_loss: 0.7120\n",
      "Epoch 303/500\n",
      "15/15 [==============================] - 1s 47ms/step - loss: 0.0193 - val_loss: 0.6512\n",
      "Epoch 304/500\n",
      "15/15 [==============================] - 1s 46ms/step - loss: 0.0118 - val_loss: 0.6788\n",
      "Epoch 305/500\n",
      "15/15 [==============================] - 1s 43ms/step - loss: 0.0293 - val_loss: 0.6685\n",
      "Epoch 306/500\n",
      "15/15 [==============================] - 1s 46ms/step - loss: 0.0180 - val_loss: 0.6536\n",
      "Epoch 307/500\n",
      "15/15 [==============================] - 1s 44ms/step - loss: 0.0238 - val_loss: 0.6840\n",
      "Epoch 308/500\n",
      "15/15 [==============================] - 1s 42ms/step - loss: 0.0247 - val_loss: 0.6655\n",
      "Epoch 309/500\n",
      "15/15 [==============================] - 1s 46ms/step - loss: 0.0186 - val_loss: 0.7105\n",
      "Epoch 310/500\n",
      "15/15 [==============================] - 1s 43ms/step - loss: 0.0173 - val_loss: 0.7059\n",
      "Epoch 311/500\n",
      "15/15 [==============================] - 1s 49ms/step - loss: 0.0123 - val_loss: 0.7153\n",
      "Epoch 312/500\n",
      "15/15 [==============================] - 1s 45ms/step - loss: 0.0134 - val_loss: 0.7072\n",
      "Epoch 313/500\n",
      "15/15 [==============================] - 1s 40ms/step - loss: 0.0162 - val_loss: 0.6887\n",
      "Epoch 314/500\n",
      "15/15 [==============================] - 1s 46ms/step - loss: 0.0089 - val_loss: 0.7082\n",
      "Epoch 315/500\n",
      "15/15 [==============================] - 1s 41ms/step - loss: 0.0075 - val_loss: 0.7132\n",
      "Epoch 316/500\n",
      "15/15 [==============================] - 1s 41ms/step - loss: 0.0076 - val_loss: 0.7114\n",
      "Epoch 317/500\n",
      "15/15 [==============================] - 1s 41ms/step - loss: 0.0069 - val_loss: 0.7119\n",
      "Epoch 318/500\n",
      "15/15 [==============================] - 1s 39ms/step - loss: 0.0070 - val_loss: 0.7100\n",
      "Epoch 319/500\n",
      "15/15 [==============================] - 1s 43ms/step - loss: 0.2017 - val_loss: 0.6640\n",
      "Epoch 320/500\n",
      "15/15 [==============================] - 1s 45ms/step - loss: 0.2259 - val_loss: 1.5808\n",
      "Epoch 321/500\n",
      "15/15 [==============================] - 1s 43ms/step - loss: 0.4805 - val_loss: 1.8280\n",
      "Epoch 322/500\n",
      "15/15 [==============================] - 1s 42ms/step - loss: 1.0366 - val_loss: 1.3544\n",
      "Epoch 323/500\n",
      "15/15 [==============================] - 1s 40ms/step - loss: 1.7334 - val_loss: 3.1033\n",
      "Epoch 324/500\n",
      "15/15 [==============================] - 1s 40ms/step - loss: 0.7015 - val_loss: 1.7204\n",
      "Epoch 325/500\n",
      "15/15 [==============================] - 1s 39ms/step - loss: 1.7766 - val_loss: 3.2995\n",
      "Epoch 326/500\n",
      "15/15 [==============================] - 1s 40ms/step - loss: 2.4348 - val_loss: 2.0156\n",
      "Epoch 327/500\n",
      "15/15 [==============================] - 1s 42ms/step - loss: 2.1734 - val_loss: 4.6630\n",
      "Epoch 328/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.7725 - val_loss: 3.4364\n",
      "Epoch 329/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.4735 - val_loss: 2.4778\n",
      "Epoch 330/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.2256 - val_loss: 4.7263\n",
      "Epoch 331/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.2111 - val_loss: 3.6750\n",
      "Epoch 332/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.1801 - val_loss: 3.3559\n",
      "Epoch 333/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.1547 - val_loss: 2.9173\n",
      "Epoch 334/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.1401 - val_loss: 3.0345\n",
      "Epoch 335/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.1219 - val_loss: 2.8294\n",
      "Epoch 336/500\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.1062 - val_loss: 2.8984\n",
      "Epoch 337/500\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.1162 - val_loss: 2.9138\n",
      "Epoch 338/500\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.1126 - val_loss: 2.7438\n",
      "Epoch 339/500\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.1186 - val_loss: 2.7731\n",
      "Epoch 340/500\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0973 - val_loss: 2.7942\n",
      "Epoch 341/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0867 - val_loss: 2.9354\n",
      "Epoch 342/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0942 - val_loss: 2.8157\n",
      "Epoch 343/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.1427 - val_loss: 2.6921\n",
      "Epoch 344/500\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0839 - val_loss: 2.8101\n",
      "Epoch 345/500\n",
      "15/15 [==============================] - 0s 34ms/step - loss: 0.0823 - val_loss: 2.8334\n",
      "Epoch 346/500\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 0.0704 - val_loss: 2.8488\n",
      "Epoch 347/500\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0572 - val_loss: 2.8413\n",
      "Epoch 348/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0637 - val_loss: 2.8975\n",
      "Epoch 349/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0657 - val_loss: 2.9639\n",
      "Epoch 350/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.3086 - val_loss: 2.9334\n",
      "Epoch 351/500\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.2188 - val_loss: 2.7327\n",
      "Epoch 352/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.1028 - val_loss: 2.7605\n",
      "Epoch 353/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0902 - val_loss: 2.7584\n",
      "Epoch 354/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0789 - val_loss: 2.7504\n",
      "Epoch 355/500\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0615 - val_loss: 2.7966\n",
      "Epoch 356/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0593 - val_loss: 2.8130\n",
      "Epoch 357/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0584 - val_loss: 2.8308\n",
      "Epoch 358/500\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 0.0557 - val_loss: 2.8777\n",
      "Epoch 359/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0560 - val_loss: 2.9116\n",
      "Epoch 360/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0519 - val_loss: 2.9194\n",
      "Epoch 361/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0546 - val_loss: 2.9385\n",
      "Epoch 362/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0498 - val_loss: 2.9788\n",
      "Epoch 363/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0446 - val_loss: 3.0246\n",
      "Epoch 364/500\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0499 - val_loss: 3.0597\n",
      "Epoch 365/500\n",
      "15/15 [==============================] - 1s 41ms/step - loss: 0.0461 - val_loss: 3.1041\n",
      "Epoch 366/500\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.1485 - val_loss: 3.0745\n",
      "Epoch 367/500\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0796 - val_loss: 2.8647\n",
      "Epoch 368/500\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 0.0778 - val_loss: 2.9387\n",
      "Epoch 369/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.1732 - val_loss: 2.9644\n",
      "Epoch 370/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0923 - val_loss: 3.1829\n",
      "Epoch 371/500\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0726 - val_loss: 3.1277\n",
      "Epoch 372/500\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0571 - val_loss: 3.0371\n",
      "Epoch 373/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0432 - val_loss: 3.0632\n",
      "Epoch 374/500\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0458 - val_loss: 3.1064\n",
      "Epoch 375/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0429 - val_loss: 3.1052\n",
      "Epoch 376/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0366 - val_loss: 3.1152\n",
      "Epoch 377/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0361 - val_loss: 3.1481\n",
      "Epoch 378/500\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0356 - val_loss: 3.1848\n",
      "Epoch 379/500\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 0.0332 - val_loss: 3.1987\n",
      "Epoch 380/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0341 - val_loss: 3.2262\n",
      "Epoch 381/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0302 - val_loss: 3.2680\n",
      "Epoch 382/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0290 - val_loss: 3.2602\n",
      "Epoch 383/500\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 0.0303 - val_loss: 3.2483\n",
      "Epoch 384/500\n",
      "15/15 [==============================] - 1s 39ms/step - loss: 0.0269 - val_loss: 3.2610\n",
      "Epoch 385/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0282 - val_loss: 3.2892\n",
      "Epoch 386/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0262 - val_loss: 3.3145\n",
      "Epoch 387/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0261 - val_loss: 3.3294\n",
      "Epoch 388/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0248 - val_loss: 3.3449\n",
      "Epoch 389/500\n",
      "15/15 [==============================] - 0s 34ms/step - loss: 0.0251 - val_loss: 3.3365\n",
      "Epoch 390/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0256 - val_loss: 3.3569\n",
      "Epoch 391/500\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 0.0228 - val_loss: 3.3765\n",
      "Epoch 392/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0234 - val_loss: 3.3861\n",
      "Epoch 393/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0229 - val_loss: 3.3918\n",
      "Epoch 394/500\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0292 - val_loss: 3.4062\n",
      "Epoch 395/500\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 0.0316 - val_loss: 3.2278\n",
      "Epoch 396/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0349 - val_loss: 3.2526\n",
      "Epoch 397/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0294 - val_loss: 3.2777\n",
      "Epoch 398/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0266 - val_loss: 3.3096\n",
      "Epoch 399/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0212 - val_loss: 3.3007\n",
      "Epoch 400/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0196 - val_loss: 3.2899\n",
      "Epoch 401/500\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 0.0185 - val_loss: 3.3226\n",
      "Epoch 402/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0211 - val_loss: 3.3233\n",
      "Epoch 403/500\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0375 - val_loss: 3.3493\n",
      "Epoch 404/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0330 - val_loss: 3.3832\n",
      "Epoch 405/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0220 - val_loss: 3.4122\n",
      "Epoch 406/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0196 - val_loss: 3.3821\n",
      "Epoch 407/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0178 - val_loss: 3.3922\n",
      "Epoch 408/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0173 - val_loss: 3.3911\n",
      "Epoch 409/500\n",
      "15/15 [==============================] - 0s 34ms/step - loss: 0.0178 - val_loss: 3.3663\n",
      "Epoch 410/500\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 0.0595 - val_loss: 3.1446\n",
      "Epoch 411/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0395 - val_loss: 3.1451\n",
      "Epoch 412/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0257 - val_loss: 3.2038\n",
      "Epoch 413/500\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0193 - val_loss: 3.2448\n",
      "Epoch 414/500\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0183 - val_loss: 3.2541\n",
      "Epoch 415/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0160 - val_loss: 3.2806\n",
      "Epoch 416/500\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 0.0168 - val_loss: 3.3037\n",
      "Epoch 417/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0170 - val_loss: 3.3340\n",
      "Epoch 418/500\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0146 - val_loss: 3.3265\n",
      "Epoch 419/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0144 - val_loss: 3.3278\n",
      "Epoch 420/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0144 - val_loss: 3.3463\n",
      "Epoch 421/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0142 - val_loss: 3.3704\n",
      "Epoch 422/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0137 - val_loss: 3.3900\n",
      "Epoch 423/500\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0136 - val_loss: 3.4142\n",
      "Epoch 424/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0127 - val_loss: 3.4198\n",
      "Epoch 425/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0139 - val_loss: 3.4114\n",
      "Epoch 426/500\n",
      "15/15 [==============================] - 0s 34ms/step - loss: 0.0139 - val_loss: 3.4272\n",
      "Epoch 427/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0139 - val_loss: 3.4318\n",
      "Epoch 428/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0117 - val_loss: 3.4451\n",
      "Epoch 429/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0120 - val_loss: 3.4667\n",
      "Epoch 430/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0114 - val_loss: 3.4669\n",
      "Epoch 431/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0112 - val_loss: 3.4493\n",
      "Epoch 432/500\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0114 - val_loss: 3.4596\n",
      "Epoch 433/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0113 - val_loss: 3.4598\n",
      "Epoch 434/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0105 - val_loss: 3.4696\n",
      "Epoch 435/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0104 - val_loss: 3.4747\n",
      "Epoch 436/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0113 - val_loss: 3.4818\n",
      "Epoch 437/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0114 - val_loss: 3.4593\n",
      "Epoch 438/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0104 - val_loss: 3.4580\n",
      "Epoch 439/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0112 - val_loss: 3.4762\n",
      "Epoch 440/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0104 - val_loss: 3.4914\n",
      "Epoch 441/500\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 0.0095 - val_loss: 3.4831\n",
      "Epoch 442/500\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0094 - val_loss: 3.4637\n",
      "Epoch 443/500\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0099 - val_loss: 3.4753\n",
      "Epoch 444/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0098 - val_loss: 3.4895\n",
      "Epoch 445/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0100 - val_loss: 3.4849\n",
      "Epoch 446/500\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 0.0095 - val_loss: 3.4808\n",
      "Epoch 447/500\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 0.0088 - val_loss: 3.5052\n",
      "Epoch 448/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0090 - val_loss: 3.4989\n",
      "Epoch 449/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0092 - val_loss: 3.4874\n",
      "Epoch 450/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0090 - val_loss: 3.5018\n",
      "Epoch 451/500\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 0.0084 - val_loss: 3.5022\n",
      "Epoch 452/500\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0089 - val_loss: 3.5004\n",
      "Epoch 453/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0087 - val_loss: 3.5026\n",
      "Epoch 454/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0086 - val_loss: 3.5081\n",
      "Epoch 455/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0087 - val_loss: 3.5164\n",
      "Epoch 456/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0082 - val_loss: 3.5137\n",
      "Epoch 457/500\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 0.0100 - val_loss: 3.5241\n",
      "Epoch 458/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0080 - val_loss: 3.5126\n",
      "Epoch 459/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0078 - val_loss: 3.5140\n",
      "Epoch 460/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0221 - val_loss: 3.4200\n",
      "Epoch 461/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0274 - val_loss: 3.5229\n",
      "Epoch 462/500\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0196 - val_loss: 3.4072\n",
      "Epoch 463/500\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 0.0232 - val_loss: 3.3477\n",
      "Epoch 464/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0190 - val_loss: 3.3355\n",
      "Epoch 465/500\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 0.0174 - val_loss: 3.4160\n",
      "Epoch 466/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0105 - val_loss: 3.4336\n",
      "Epoch 467/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0102 - val_loss: 3.4475\n",
      "Epoch 468/500\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0095 - val_loss: 3.4600\n",
      "Epoch 469/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0083 - val_loss: 3.4799\n",
      "Epoch 470/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0090 - val_loss: 3.5285\n",
      "Epoch 471/500\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0098 - val_loss: 3.5375\n",
      "Epoch 472/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0095 - val_loss: 3.5293\n",
      "Epoch 473/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0081 - val_loss: 3.5091\n",
      "Epoch 474/500\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0077 - val_loss: 3.5127\n",
      "Epoch 475/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0072 - val_loss: 3.5235\n",
      "Epoch 476/500\n",
      "15/15 [==============================] - 1s 33ms/step - loss: 0.0079 - val_loss: 3.5246\n",
      "Epoch 477/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0068 - val_loss: 3.5250\n",
      "Epoch 478/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0079 - val_loss: 3.5250\n",
      "Epoch 479/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0077 - val_loss: 3.5308\n",
      "Epoch 480/500\n",
      "15/15 [==============================] - 1s 33ms/step - loss: 0.0069 - val_loss: 3.5391\n",
      "Epoch 481/500\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0064 - val_loss: 3.5407\n",
      "Epoch 482/500\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 0.0077 - val_loss: 3.5418\n",
      "Epoch 483/500\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 0.0063 - val_loss: 3.5582\n",
      "Epoch 484/500\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0063 - val_loss: 3.5458\n",
      "Epoch 485/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0062 - val_loss: 3.5643\n",
      "Epoch 486/500\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 0.0061 - val_loss: 3.5861\n",
      "Epoch 487/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0074 - val_loss: 3.5648\n",
      "Epoch 488/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0068 - val_loss: 3.5699\n",
      "Epoch 489/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0068 - val_loss: 3.5780\n",
      "Epoch 490/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0058 - val_loss: 3.5828\n",
      "Epoch 491/500\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0062 - val_loss: 3.5657\n",
      "Epoch 492/500\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 0.0056 - val_loss: 3.5832\n",
      "Epoch 493/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0056 - val_loss: 3.5888\n",
      "Epoch 494/500\n",
      "15/15 [==============================] - 0s 34ms/step - loss: 0.0055 - val_loss: 3.5908\n",
      "Epoch 495/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0055 - val_loss: 3.5976\n",
      "Epoch 496/500\n",
      "15/15 [==============================] - 0s 34ms/step - loss: 0.0055 - val_loss: 3.5982\n",
      "Epoch 497/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0050 - val_loss: 3.6013\n",
      "Epoch 498/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0050 - val_loss: 3.6067\n",
      "Epoch 499/500\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 0.0065 - val_loss: 3.6139\n",
      "Epoch 500/500\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0056 - val_loss: 3.5975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2765286c670>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Anchor = X_train[:,0,:].reshape(-1,768,1)\n",
    "Positive = X_train[:,1,:].reshape(-1,768,1)\n",
    "Negative = X_train[:,2,:].reshape(-1,768,1)\n",
    "Anchor_test = X_test[:,0,:].reshape(-1,768,1)\n",
    "Positive_test = X_test[:,1,:].reshape(-1,768,1)\n",
    "Negative_test = X_test[:,2,:].reshape(-1,768,1)\n",
    "\n",
    "Y_dummy = np.empty((Anchor.shape[0],300))\n",
    "Y_dummy2 = np.empty((Anchor_test.shape[0],1))\n",
    "\n",
    "model.fit([Anchor,Positive,Negative],y=Y_dummy,validation_data=([Anchor_test,Positive_test,Negative_test],Y_dummy2), batch_size=512, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 768, 1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_ready_data = loaded_ast_embeddings.reshape(-1,768,1)\n",
    "predict_ready_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "predictedModel = model.predict([predict_ready_data, predict_ready_data, predict_ready_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2304)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictedModel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputAnchor = predictedModel[:, :768]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.30531593e-02,  7.21399009e-01,  5.73245108e-01,  3.63488466e-01,\n",
       "        8.37056518e-01, -5.88767350e-01,  1.34191561e+00,  1.34519681e-01,\n",
       "       -1.37897059e-01,  9.78766143e-01, -8.29323649e-01,  6.36339664e-01,\n",
       "       -7.44484663e-02,  7.21939802e-01, -5.63872218e-01,  2.30193630e-01,\n",
       "       -6.67314708e-01, -8.93377364e-01,  1.23766375e+00, -2.69494474e-01,\n",
       "       -1.81013867e-02, -4.71434116e-01, -5.78870118e-01, -2.86064565e-01,\n",
       "        1.19328403e+00,  4.56193030e-01, -2.28490576e-01, -6.42638862e-01,\n",
       "       -1.71555269e+00,  1.67028338e-01, -5.70156753e-01, -3.65860879e-01,\n",
       "        5.34271240e-01,  1.06910303e-01, -1.77924946e-01,  3.41371626e-01,\n",
       "        2.21525356e-01,  1.26615763e-01,  4.47818846e-01,  5.47480583e-02,\n",
       "        4.31222349e-01, -3.51558715e-01,  7.44501591e-01,  4.24757693e-03,\n",
       "       -1.08954608e+00, -6.50323153e-01,  1.32380739e-01,  1.81998983e-01,\n",
       "        4.80778724e-01, -2.93770969e-01, -1.32629931e+00,  1.10504031e+00,\n",
       "        1.13006413e+00,  2.43111789e-01, -4.24457043e-02, -1.25869119e+00,\n",
       "        6.86672091e-01, -1.64312315e+00, -6.86543528e-03, -2.60999262e-01,\n",
       "       -8.40703666e-01,  1.94754437e-01,  1.00551057e+00,  6.86581194e-01,\n",
       "       -2.09462836e-01,  2.83968121e-01,  2.30604663e-01, -2.01068223e-01,\n",
       "       -1.57360017e+00, -4.94653136e-01,  1.76228389e-01, -3.19564283e-01,\n",
       "       -7.35188484e-01, -2.54515827e-01, -8.28558207e-02, -2.07824688e-02,\n",
       "       -1.40890703e-01,  4.00107324e-01,  8.22852492e-01,  5.49082816e-01,\n",
       "        4.39675242e-01,  1.76876321e-01,  4.87655222e-01, -4.00969356e-01,\n",
       "        8.61954033e-01,  2.58781701e-01,  8.80196452e-01,  6.90940976e-01,\n",
       "       -9.85980630e-01,  5.77780187e-01,  8.08768347e-03,  1.66128504e+00,\n",
       "       -7.30416536e-01, -6.93148971e-02, -3.79840165e-01, -1.13248795e-01,\n",
       "       -9.76724774e-02, -5.46772452e-03,  4.96090442e-01,  5.67299843e-01,\n",
       "       -6.28213286e-01, -1.05105412e+00, -1.55029163e-01,  1.16651550e-01,\n",
       "       -4.01138902e-01, -5.41157484e-01, -5.79857349e-01,  1.76956981e-01,\n",
       "       -9.51967418e-01,  1.30569565e+00,  4.17496622e-01,  2.39226833e-01,\n",
       "       -4.75356206e-02,  3.63585830e-01,  5.51349670e-03, -3.78957689e-01,\n",
       "       -9.07721341e-01,  5.39704621e-01,  8.60208988e-01,  1.38516080e+00,\n",
       "        3.49742234e-01,  2.67496943e-01,  3.37018748e-03,  4.93813246e-01,\n",
       "        1.05164170e+00,  3.49160075e-01, -4.61710215e-01,  8.08924854e-01,\n",
       "       -3.57959121e-01,  2.59307861e-01,  6.98219717e-01,  1.38287663e+00,\n",
       "        1.14966393e+00, -6.77595019e-01, -6.50914192e-01, -1.45483799e-02,\n",
       "       -5.14674723e-01, -5.86952388e-01, -2.95112357e-02, -1.51264980e-01,\n",
       "       -3.73696864e-01, -8.78731385e-02,  1.25451222e-01, -3.99054773e-02,\n",
       "       -1.05607346e-01, -3.22768778e-01,  6.06571175e-02,  5.46660602e-01,\n",
       "       -1.96821705e-01,  1.19013858e+00, -8.36305261e-01,  9.78412747e-01,\n",
       "       -1.29050958e+00, -6.63254797e-01, -9.75216746e-01, -1.13496959e+00,\n",
       "       -6.97717071e-01,  1.58719018e-01, -5.12014749e-03,  3.26908141e-01,\n",
       "        2.08910042e-03,  1.17168498e+00, -3.22914183e-01,  5.32013535e-01,\n",
       "        1.83159336e-01, -9.11935210e-01,  3.97384703e-01, -8.50340366e-01,\n",
       "       -1.13108866e-01, -7.29892552e-01, -7.29803622e-01, -1.57265633e-01,\n",
       "        6.11322999e-01, -3.20160896e-01,  5.51302552e-01,  1.92855876e-02,\n",
       "        2.89786667e-01, -4.59377885e-01,  4.85976994e-01, -8.34298253e-01,\n",
       "       -1.15742283e-02,  3.68532777e-01, -3.06375444e-01, -3.63204256e-02,\n",
       "       -1.03865981e+00, -3.47471610e-02,  1.09378263e-01, -1.42596796e-01,\n",
       "        5.75852394e-01, -1.16475321e-01, -2.41992339e-01, -1.28377169e-01,\n",
       "        5.84089339e-01, -4.77536827e-01, -2.13568825e-02,  4.16896999e-01,\n",
       "       -9.17937160e-01,  1.03160930e+00, -7.71492302e-01,  4.87939328e-01,\n",
       "        1.02031183e+00,  2.07653433e-01,  3.53551328e-01, -8.26955438e-01,\n",
       "        1.49439439e-01,  6.75067961e-01,  4.79821116e-02,  4.06698227e-01,\n",
       "       -8.39776874e-01, -5.16575217e-01, -1.06646037e+00,  3.67613316e-01,\n",
       "        6.25564856e-03,  8.35959613e-01,  8.45044076e-01, -7.01799572e-01,\n",
       "        5.03562808e-01, -1.08767819e+00,  6.08119965e-01,  6.83402658e-01,\n",
       "        3.78051847e-01, -4.72442806e-01,  3.57375473e-01,  8.58227313e-01,\n",
       "       -9.52918828e-01,  8.07925165e-01,  1.28578141e-01,  1.29398629e-01,\n",
       "        3.29703659e-01,  4.55416113e-01,  3.56873274e-01, -1.48323134e-01,\n",
       "       -7.31937408e-01,  2.88233876e-01,  1.26346782e-01,  2.79545963e-01,\n",
       "       -7.50221610e-01, -6.71519712e-02, -1.99849367e-01,  1.16644844e-01,\n",
       "        1.17988765e+00,  7.26892591e-01,  1.76756322e-01,  5.43410897e-01,\n",
       "       -3.27823967e-01, -1.18693829e+00, -3.95399854e-02,  1.64693128e-02,\n",
       "       -4.37346101e-01,  2.84491897e-01, -1.24888346e-01, -4.11941171e-01,\n",
       "        4.94277656e-01, -4.63277191e-01,  2.49444753e-01,  4.20652807e-01,\n",
       "       -4.88523543e-01, -1.84270054e-01, -6.17661417e-01,  7.25618303e-01,\n",
       "       -4.94904667e-01, -5.51141381e-01, -3.14263135e-01,  1.33541644e-01,\n",
       "        3.62805635e-01,  2.24615023e-01,  6.14264727e-01,  1.02795911e+00,\n",
       "       -4.88739014e-01,  1.41086608e-01, -7.60315001e-01, -4.36916441e-01,\n",
       "       -1.41653776e-01,  3.55248675e-02, -7.15807855e-01, -1.22209720e-01,\n",
       "       -4.83277589e-02,  4.13248628e-01, -4.00246531e-01,  1.10477038e-01,\n",
       "       -1.18504834e+00,  8.21682289e-02, -1.26882648e+00, -7.18137324e-02,\n",
       "        9.21702147e-01, -1.36009201e-01, -8.89387131e-02, -1.03663063e+00,\n",
       "       -1.39549708e+00,  3.39531302e-01, -4.84060407e-01, -6.42578423e-01,\n",
       "       -9.05384660e-01,  2.51745991e-03,  4.42700349e-02, -5.14390469e-01,\n",
       "        3.90200257e-01, -6.71598971e-01,  7.15710968e-02, -5.96635938e-01,\n",
       "        8.14215243e-02, -8.39293376e-02, -1.05040693e+00,  7.70217001e-01,\n",
       "        7.82547519e-02, -8.00167799e-01,  1.56273380e-01,  4.96953458e-01,\n",
       "       -6.18534446e-01,  1.87474132e-01, -2.89148897e-01,  1.30074799e-01,\n",
       "        3.16481501e-01,  8.13057899e-01,  1.37071401e-01,  6.80757880e-01,\n",
       "       -6.92690536e-02,  4.21474665e-01, -3.13095599e-02, -1.05136561e+00,\n",
       "       -4.68153149e-01,  2.86567926e-01,  6.86369538e-02,  3.47100824e-01,\n",
       "        3.32706183e-01,  3.81417632e-01, -2.40687743e-01,  4.26640034e-01,\n",
       "       -5.73752224e-01, -3.15856822e-02,  4.75199193e-01, -1.40720356e+00,\n",
       "        4.00977224e-01,  3.56557250e-01,  1.05957814e-01,  8.49963605e-01,\n",
       "       -3.45417947e-01, -1.33640364e-01,  4.13581342e-01, -5.46492159e-01,\n",
       "        4.46282268e-01, -4.39847291e-01, -5.21583974e-01,  5.30928433e-01,\n",
       "        3.48365426e-01, -5.16869247e-01,  2.71229595e-01, -7.07625085e-03,\n",
       "        2.36841336e-01,  2.55573690e-01,  3.70077193e-01, -3.59538585e-01,\n",
       "        5.16215920e-01,  9.74602461e-01, -1.63190559e-01, -6.11918449e-01,\n",
       "       -8.67887661e-02,  1.89558938e-01, -1.38521326e+00,  1.63436607e-01,\n",
       "        1.97415382e-01,  9.17946279e-01,  5.65734208e-02, -6.38382196e-01,\n",
       "       -5.21489620e-01,  2.53320094e-02, -1.26357600e-01,  6.03180170e-01,\n",
       "        6.52186811e-01, -5.89438736e-01,  1.88690990e-01, -4.78119791e-01,\n",
       "       -2.58080930e-01, -9.03900862e-01, -6.33568168e-01,  1.54754017e-02,\n",
       "        5.58431208e-01, -2.91432112e-01, -4.52590108e-01, -8.22143495e-01,\n",
       "        1.15219261e-02, -7.65813172e-01,  2.01359808e-01, -1.00873627e-01,\n",
       "       -4.71110821e-01, -6.10040165e-02,  3.17393452e-01, -2.46892184e-01,\n",
       "       -3.36726665e-01,  2.39356548e-01,  2.94300258e-01,  8.65057170e-01,\n",
       "        2.20013604e-01,  8.85333493e-02,  1.82771474e-01, -1.32444787e+00,\n",
       "        3.39783013e-01,  6.15230680e-01, -4.00114864e-01,  1.20369516e-01,\n",
       "        9.91484448e-02, -9.72954556e-02,  2.77518839e-01, -7.19009042e-02,\n",
       "        4.47317302e-01, -4.95262951e-01,  1.06912762e-01, -4.78860408e-01,\n",
       "        5.93035221e-01, -2.03631774e-01,  6.23249292e-01, -1.26259315e+00,\n",
       "        4.42762077e-02, -3.26631069e-01, -4.61537540e-01,  1.12664628e+00,\n",
       "       -3.68686989e-02, -6.98562264e-01, -9.70332146e-01,  8.18709493e-01,\n",
       "        2.99545765e-01, -6.91289842e-01, -3.74605924e-01,  2.43744180e-01,\n",
       "        1.59607679e-01,  9.87062633e-01, -5.46432257e-01, -6.45077467e-01,\n",
       "       -6.64503634e-01,  6.09782100e-01,  4.25604790e-01, -6.81717023e-02,\n",
       "       -7.21753776e-01, -5.09967566e-01,  5.08965731e-01,  6.06090575e-02,\n",
       "        2.72761226e-01,  1.17763262e-02,  1.35606557e-01, -3.46122622e-01,\n",
       "       -4.52119291e-01, -1.55860066e+00,  1.33701358e-02,  1.11914344e-01,\n",
       "       -1.20964967e-01,  1.28319025e-01, -4.39518154e-01, -3.34460676e-01,\n",
       "        5.28467476e-01, -5.91124833e-01,  2.62302488e-01, -3.02888244e-01,\n",
       "        1.31375477e-01,  2.52390295e-01,  1.01176739e+00, -2.84961164e-02,\n",
       "       -3.91831756e-01, -6.10487938e-01, -1.98623896e-01, -4.76972461e-01,\n",
       "        6.27904713e-01,  1.01357825e-01,  5.66423312e-03, -5.30160844e-01,\n",
       "        8.56959641e-01,  1.00992906e+00, -4.57355738e-01, -2.79006749e-01,\n",
       "        1.23979017e-01,  3.17170709e-01,  6.04219735e-02, -2.92677999e-01,\n",
       "        8.96151294e-04,  3.28643769e-01, -2.62446433e-01, -3.88828069e-02,\n",
       "       -4.33259398e-01,  6.19499505e-01, -1.51629627e-01, -1.62854165e-01,\n",
       "       -2.55893558e-01,  1.01445985e+00,  6.56445265e-01,  5.29936671e-01,\n",
       "       -4.32574183e-01, -9.67207968e-01,  4.69263852e-01,  1.20884132e+00,\n",
       "        8.88328671e-01, -8.28591406e-01,  2.33075365e-01, -4.61228013e-01,\n",
       "       -1.29491314e-01, -6.78599834e-01, -8.68793279e-02,  1.41661599e-01,\n",
       "       -2.19997942e-01, -7.32556105e-01,  1.09161481e-01, -9.19230878e-01,\n",
       "        1.74809265e+00,  5.30804396e-01, -1.85818508e-01, -5.11834264e-01,\n",
       "        1.08921885e+00,  7.02562213e-01, -2.03624487e-01, -3.77851933e-01,\n",
       "        4.66066450e-02,  3.33918542e-01, -8.67755473e-01, -3.71166706e-01,\n",
       "        3.25998403e-02,  1.77682683e-01, -2.63631344e-01, -7.06934929e-01,\n",
       "        6.33501947e-01,  2.66593963e-01, -3.32189910e-02, -9.82248485e-01,\n",
       "       -2.15937153e-01, -5.01361676e-02,  7.66301990e-01, -1.48688936e+00,\n",
       "        6.39321089e-01, -9.07348812e-01,  3.40465680e-02,  4.40624416e-01,\n",
       "        1.17077209e-01, -2.49259338e-01, -3.11165780e-01,  2.99154937e-01,\n",
       "       -4.47222799e-01,  2.07787797e-01, -6.06665015e-01,  6.73050284e-02,\n",
       "        1.07848622e-01,  7.10272551e-01,  3.66920203e-01, -6.83878541e-01,\n",
       "       -5.99707570e-03,  1.85127929e-01,  3.84645611e-01, -4.55273777e-01,\n",
       "       -5.99933147e-01,  5.84435821e-01, -1.13787286e-01, -6.62996829e-01,\n",
       "        2.49946281e-01, -1.00350030e-01,  4.33127880e-01, -3.46106499e-01,\n",
       "        1.58930719e-01, -6.24723911e-01, -4.31560665e-01,  8.63450244e-02,\n",
       "       -5.73172048e-02,  4.98830974e-01, -1.09893598e-01,  4.53414738e-01,\n",
       "        3.53314430e-01,  3.70558873e-02,  3.97476517e-02,  2.34130159e-01,\n",
       "        1.82260230e-01, -4.58521366e-01, -9.44408178e-01,  2.18045488e-01,\n",
       "       -2.41393656e-01, -5.99223733e-01,  3.32988977e-01, -3.63439590e-01,\n",
       "        2.88923711e-01,  8.63483787e-01, -8.83688629e-01, -2.50752121e-01,\n",
       "        7.28369176e-01, -6.62133932e-01, -7.08896935e-01, -4.48142923e-02,\n",
       "        1.10541832e+00,  1.25238228e+00, -2.71331170e-03, -4.46823418e-01,\n",
       "       -1.79934859e-01, -1.79788381e-01,  3.77799511e-01, -4.83706325e-01,\n",
       "       -2.41292313e-01, -4.42991376e-01, -6.87781274e-01,  7.73703586e-03,\n",
       "        3.78270060e-01,  3.58401179e-01,  3.90098542e-01,  4.48597252e-01,\n",
       "        5.23805737e-01,  6.48796976e-01, -4.11969304e-01, -2.16645956e-01,\n",
       "        3.87457639e-01, -5.34365535e-01, -1.53651965e+00,  1.00510895e+00,\n",
       "       -8.92770827e-01,  6.18131578e-01,  4.14299399e-01,  5.01100659e-01,\n",
       "       -1.16807127e+00,  8.11692357e-01,  5.47587693e-01, -9.17196512e-01,\n",
       "        6.95379078e-02, -1.95414305e-01, -4.66203131e-02,  3.00740674e-02,\n",
       "       -3.28664631e-01,  4.00093168e-01,  2.42402047e-01,  5.17436326e-01,\n",
       "        3.73731583e-01, -3.21918070e-01, -6.34663999e-01, -6.26225770e-01,\n",
       "        2.31017143e-01, -7.94725120e-01, -1.63553208e-01, -2.45448604e-01,\n",
       "       -1.50528699e-01,  2.16344893e-01,  9.33095694e-01,  5.48485756e-01,\n",
       "        6.48806512e-01,  6.81010365e-01,  1.21911831e-01,  5.85026741e-01,\n",
       "       -6.05069876e-01, -7.42161393e-01,  5.16068220e-01, -1.02472067e+00,\n",
       "       -6.72428370e-01, -3.65904607e-02, -4.57360893e-01, -7.62980878e-02,\n",
       "        6.89428508e-01, -2.71204095e-02,  6.69018105e-02,  4.51154441e-01,\n",
       "        5.38909554e-01,  2.22290233e-01, -2.38215804e-01, -7.33513292e-03,\n",
       "       -5.11881784e-02,  4.00542229e-01,  5.81609845e-01,  6.03202760e-01,\n",
       "        1.57594740e-01,  4.69871819e-01, -1.97744027e-01,  4.24439549e-01,\n",
       "        6.06630564e-01, -4.28199857e-01,  1.56667791e-02,  3.53922248e-01,\n",
       "        1.24470282e+00,  4.16972190e-01,  5.93656301e-01, -3.81624073e-01,\n",
       "       -1.09962977e-01, -2.95092106e-01, -4.36568320e-01,  2.05840796e-01,\n",
       "        5.74405417e-02, -3.01857799e-01,  1.06943071e+00, -3.42348479e-02,\n",
       "        8.78996849e-01, -5.29758632e-01, -4.13617566e-02, -1.08153737e+00,\n",
       "       -3.46004456e-01,  7.89237842e-02, -4.40791786e-01, -6.54839277e-01,\n",
       "        1.04325557e+00,  6.12077177e-01, -1.14309585e+00, -2.84524746e-02,\n",
       "        2.51512378e-02,  1.07682735e-01,  3.75997037e-01, -2.61250019e-01,\n",
       "       -1.38750434e-01, -1.14641917e+00,  4.92327660e-01, -3.55007857e-01,\n",
       "       -6.94584250e-01,  5.06319165e-01, -7.62671769e-01, -1.48240075e-01,\n",
       "       -1.17184746e+00,  1.33580556e-02, -2.66305059e-01, -1.36866391e-01,\n",
       "       -9.14705619e-02,  5.64892769e-01, -1.98689684e-01, -2.02439725e-02,\n",
       "       -3.93149763e-01, -1.78373948e-01, -1.22801162e-01,  1.53158295e+00,\n",
       "       -6.27581596e-01,  8.56359601e-02, -1.10685050e+00,  6.17428303e-01,\n",
       "       -2.10953370e-01,  8.89818221e-02,  3.23726535e-01, -4.26322147e-02,\n",
       "        4.98890400e-01, -4.70334113e-01, -9.49848413e-01, -4.43281889e-01,\n",
       "       -3.94379407e-01,  4.94735725e-02,  2.18457237e-01,  1.23096295e-01,\n",
       "       -4.70379293e-01, -2.46960893e-01,  1.38140529e-01,  5.58473244e-02,\n",
       "       -5.64474821e-01,  5.80143869e-01,  5.97504616e-01, -1.92205682e-01,\n",
       "       -4.29739326e-01, -3.92578781e-01,  7.29567790e-03, -1.04357302e+00,\n",
       "        4.17448074e-01,  3.57263356e-01, -8.83839428e-01, -4.03187364e-01,\n",
       "        4.11276311e-01,  6.70804501e-01,  4.28397566e-01,  4.18742329e-01,\n",
       "       -4.37026381e-01,  7.01185465e-01,  4.57453758e-01,  6.37437627e-02,\n",
       "        3.36192012e-01,  4.31765392e-02, -5.84626436e-01, -6.11416042e-01,\n",
       "       -2.92587966e-01, -6.43284023e-01,  2.40435243e-01, -3.19784373e-01,\n",
       "       -6.02600217e-01,  1.17749438e-01, -2.48161063e-01, -1.73039287e-01,\n",
       "       -4.86969352e-02, -8.23293567e-01, -9.31049824e-01,  2.96983957e-01])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.04101956e+00, -9.22686979e-02, -3.32211524e-01,  1.99673578e-01,\n",
       "       -7.69471843e-03,  1.52403247e+00, -6.44789875e-01, -1.07664216e+00,\n",
       "        1.70765072e-01, -3.68559241e-01,  8.93172979e-01, -4.50514793e-01,\n",
       "       -1.19771802e+00,  9.17791545e-01, -3.15983683e-01, -3.66473049e-01,\n",
       "        2.45225072e+00, -2.18654588e-01, -6.56809628e-01, -1.06306446e+00,\n",
       "        4.48867679e-01,  2.28784084e-01, -6.71260715e-01, -6.20196402e-01,\n",
       "        1.05643892e+00, -3.80660236e-01,  6.84267819e-01,  2.03097731e-01,\n",
       "       -3.18890810e+00,  2.19999269e-01,  1.27515495e-01,  8.27168643e-01,\n",
       "       -1.45578170e+00,  1.46845973e+00,  1.15233517e+00,  6.39650106e-01,\n",
       "       -6.14069887e-02, -1.87364995e+00,  1.63796377e+00,  2.28171253e+00,\n",
       "        1.25179482e+00, -4.96792555e-01, -1.20025134e+00,  7.07903579e-02,\n",
       "        2.65869856e-01, -2.40666449e-01,  7.98124373e-02, -1.76189983e+00,\n",
       "        7.71629205e-03, -1.07935655e+00,  9.15472984e-01, -2.17511086e-03,\n",
       "       -1.93477631e+00,  3.29653770e-01,  8.62717908e-03, -1.44017541e+00,\n",
       "       -8.87900963e-02,  2.17535210e+00,  7.51448497e-02,  2.06315684e+00,\n",
       "       -4.20710415e-01,  1.82058012e+00,  9.73402202e-01,  4.67656016e-01,\n",
       "        1.33593464e+00, -9.88439202e-01,  5.32367527e-02, -4.69798855e-02,\n",
       "       -2.45620962e-02, -4.73387748e-01,  5.15479386e-01,  1.34697366e+00,\n",
       "        4.76952314e-01,  4.48019683e-01,  4.83189255e-01,  2.12976980e+00,\n",
       "        8.75397772e-02,  3.83271188e-01,  9.08162355e-01,  9.51493382e-01,\n",
       "        5.85009992e-01,  1.10363138e+00,  2.82245204e-02, -2.68990338e-01,\n",
       "        1.48207899e-02, -5.59296489e-01,  3.19683671e-01, -9.22565639e-01,\n",
       "        1.17429245e+00, -1.51655197e+00, -1.54576719e+00, -1.11339889e-01,\n",
       "       -5.30977309e-01, -6.85623527e-01,  1.08476949e+00,  2.12085247e+00,\n",
       "        8.94553483e-01,  1.14126794e-01, -2.16729298e-01,  6.39732368e-03,\n",
       "       -6.79197848e-01,  1.60356775e-01,  2.24227953e+00,  9.23069060e-01,\n",
       "       -2.43358850e-01,  1.14278948e+00,  2.75680989e-01,  4.78117093e-02,\n",
       "       -1.24246597e+00,  1.44941136e-01, -7.01461911e-01, -1.21770859e+00,\n",
       "        2.11732554e+00,  9.05179739e-01,  4.71615285e-01,  3.62751931e-01,\n",
       "       -6.90364242e-01, -1.87185723e-02, -3.47390592e-01,  6.15741491e-01,\n",
       "        1.21160984e+00,  7.65527606e-01, -3.25543657e-02,  2.26001048e+00,\n",
       "        4.84913945e-01,  1.01002645e+00,  6.13646448e-01,  9.56134975e-01,\n",
       "       -2.50663370e-01, -1.65304124e+00, -7.75268674e-01,  7.00866163e-01,\n",
       "        2.70767236e+00, -2.50711948e-01, -1.38661849e+00, -3.08794260e-01,\n",
       "       -4.13302481e-01,  8.57015789e-01,  9.23324406e-01, -1.32297266e+00,\n",
       "        9.42036092e-01,  1.11347866e+00,  1.10339332e+00,  4.42290127e-01,\n",
       "       -6.43505901e-03, -5.54805994e-01, -7.81535566e-01,  1.20089555e+00,\n",
       "        5.82325876e-01, -2.73114324e-01,  2.15212798e+00,  1.83093405e+00,\n",
       "       -2.20357046e-01,  3.33166987e-01,  3.57528061e-01,  2.91586816e-01,\n",
       "        6.01986468e-01, -1.01301897e+00, -1.28228652e+00,  1.36583626e+00,\n",
       "        2.70860106e-01, -9.21936095e-01, -9.49715912e-01,  5.08643270e-01,\n",
       "       -6.42824411e-01,  1.56612664e-01, -1.01755130e+00, -1.78453863e+00,\n",
       "        6.89371586e-01, -2.28821144e-01,  2.29245052e-02,  7.29004517e-02,\n",
       "       -4.04374123e-01, -4.29502070e-01,  2.24284559e-01, -1.09408760e+00,\n",
       "       -5.73521197e-01,  3.29248935e-01,  1.36179721e+00,  4.86788124e-01,\n",
       "       -7.52910078e-01,  1.46242768e-01, -5.17092466e-01,  1.77856788e-01,\n",
       "       -1.84488332e+00,  1.51840997e+00,  1.17199123e+00, -3.09852928e-01,\n",
       "        9.25504509e-03, -4.23774660e-01, -1.40621662e-01, -2.08047241e-01,\n",
       "        8.37473333e-01,  9.26962376e-01, -2.55261272e-01,  2.24723235e-01,\n",
       "        6.22633159e-01,  6.57570124e-01, -1.34258166e-01, -9.69209969e-01,\n",
       "        1.78828418e-01,  9.28306356e-02,  4.15785015e-01, -6.11635506e-01,\n",
       "       -1.27068922e-01,  6.09651983e-01, -5.72392106e-01, -2.35357547e+00,\n",
       "       -1.59272778e+00,  2.35688233e+00, -6.14527427e-03, -1.98115504e+00,\n",
       "        5.21702766e-01,  1.61436215e-01,  5.34753799e-01,  1.75818777e+00,\n",
       "        5.78382790e-01, -3.93872529e-01,  1.37778795e+00, -7.97024548e-01,\n",
       "       -3.75827104e-01, -5.59948161e-02,  1.94373168e-02, -4.73066300e-01,\n",
       "       -4.95364785e-01, -3.44901800e-01, -3.10421530e-02,  3.97613287e-01,\n",
       "       -1.18255746e+00, -2.96001118e-02, -1.93154919e+00,  1.18661016e-01,\n",
       "        4.19944316e-01,  1.91153899e-01, -4.96555239e-01,  8.65527213e-01,\n",
       "       -4.38809931e-01, -2.88916416e-02, -1.30427980e+00,  5.98833025e-01,\n",
       "        3.10244709e-01,  1.46757960e+00, -3.85255486e-01,  1.59780288e+00,\n",
       "       -4.46737230e-01, -8.18396449e-01,  5.79332173e-01, -8.62015426e-01,\n",
       "       -9.49755967e-01,  5.59953451e-01, -8.69332328e-02,  6.91858292e-01,\n",
       "       -1.12725651e+00,  4.63483810e-01, -9.26451743e-01, -1.44079208e+00,\n",
       "       -1.91325948e-01, -4.33978811e-02,  1.74598432e+00, -4.03940290e-01,\n",
       "       -1.04793930e+00,  2.69417554e-01,  2.80855875e-02,  2.64213443e-01,\n",
       "        9.66284633e-01,  2.01202676e-01, -1.93669554e-02,  3.97415400e-01,\n",
       "        3.85412239e-02, -2.09682417e+00,  1.27834201e+00, -1.20249379e+00,\n",
       "        7.93565869e-01, -6.70061529e-01,  9.61637676e-01,  1.45299745e+00,\n",
       "       -1.36428154e+00,  4.26388115e-01, -1.63303411e+00,  5.57479084e-01,\n",
       "        2.83093482e-01,  5.91810107e-01, -5.70605993e-02,  4.25130874e-02,\n",
       "        4.26852912e-01, -6.67888045e-01, -8.42139184e-01,  8.94889891e-01,\n",
       "        1.47315547e-01, -1.11475000e-02, -1.46945310e+00,  3.83050323e-01,\n",
       "       -1.30569679e-03, -2.31223763e-04,  1.22189713e+00,  2.42659941e-01,\n",
       "        8.23347569e-01, -4.99116361e-01,  8.87777209e-01,  4.84410137e-01,\n",
       "        2.87454575e-01, -5.04881799e-01,  8.18486869e-01,  4.48145032e-01,\n",
       "        3.57240736e-01,  7.57055879e-02, -5.17448902e-01, -1.43821537e+00,\n",
       "       -2.61478454e-01,  2.13205171e+00,  1.45007253e+00,  8.03297311e-02,\n",
       "        2.15803766e+00, -3.42055857e-01,  4.07144338e-01,  6.80485487e-01,\n",
       "       -7.27716744e-01,  1.34849942e+00,  1.67847853e-02,  1.44482183e+00,\n",
       "        1.05892777e-01,  9.43002284e-01,  8.73445868e-01, -7.89310098e-01,\n",
       "       -1.05674088e+00, -2.01785758e-01,  8.53002846e-01, -1.54965067e+00,\n",
       "       -8.71939242e-01, -6.06925547e-01, -2.15939379e+00,  1.29860032e+00,\n",
       "       -5.79023361e-01, -1.65127695e+00, -3.59971404e-01,  1.88824490e-01,\n",
       "        1.10490835e+00, -6.32329732e-02,  1.19701251e-01,  3.40461373e-01,\n",
       "        7.99865127e-02, -6.98918641e-01,  1.50622952e+00,  1.26174927e+00,\n",
       "        8.62793207e-01, -6.12369418e-01, -2.04635501e+00,  8.91889811e-01,\n",
       "       -7.64473081e-01, -8.70929599e-01, -2.32842541e+00,  8.50845397e-01,\n",
       "        6.23896956e-01,  5.19216895e-01, -4.98683929e-01, -2.60297954e-01,\n",
       "        2.12190437e+00, -4.20505494e-01,  1.93297064e+00,  5.46549439e-01,\n",
       "        1.36369634e+00,  1.65978837e+00,  3.50961208e-01, -1.02123952e+00,\n",
       "       -1.76121902e+00, -6.45053247e-03, -1.28480136e+00, -1.58685517e+00,\n",
       "       -3.98589671e-01,  5.97318232e-01, -2.51261014e-02,  1.14309454e+00,\n",
       "        1.17029369e+00, -1.03731491e-01,  1.60004079e+00,  5.59839904e-01,\n",
       "       -1.06164861e+00, -1.63118017e+00,  1.00627398e+00, -3.96884322e-01,\n",
       "        1.04015529e+00,  9.18868244e-01,  1.23096853e-01, -9.75917041e-01,\n",
       "       -9.92902890e-02,  1.39864576e+00,  7.04280198e-01,  7.61487722e-01,\n",
       "       -4.23354000e-01,  1.27944040e+00, -8.35558414e-01, -1.59680259e+00,\n",
       "        1.36101544e+00,  1.77787268e+00, -1.99983522e-01, -1.45673025e+00,\n",
       "       -1.37600267e+00,  7.15321004e-01, -8.18557501e-01, -2.05537415e+00,\n",
       "       -1.31809652e+00,  2.09814101e-01,  4.54878837e-01,  3.46603155e-01,\n",
       "       -4.60651845e-01, -5.76602578e-01, -2.82166910e+00,  3.99765253e-01,\n",
       "        2.21816942e-01,  2.18876749e-01,  1.01957941e+00,  2.35447705e-01,\n",
       "        1.54918873e+00, -1.63441586e+00,  5.71199097e-02, -9.35701311e-01,\n",
       "        1.13587105e+00,  7.68549681e-01, -6.46350384e-02, -2.31195140e+00,\n",
       "        8.36246073e-01, -1.58051461e-01,  6.78003430e-01,  2.03189343e-01,\n",
       "        1.02189040e+00, -5.56739986e-01, -7.35828042e-01,  4.90214914e-01,\n",
       "        1.24410164e+00,  2.27983332e+00,  1.51171649e+00,  6.18920684e-01,\n",
       "        5.63652694e-01, -1.89781678e+00, -6.94756210e-01,  2.17070311e-01,\n",
       "       -1.48185492e+00, -1.96396661e+00, -1.31859839e-01,  2.19127631e+00,\n",
       "       -2.30788183e+00,  4.86450285e-01, -3.34634185e-02,  4.98713911e-01,\n",
       "       -1.59099817e+00,  4.12626922e-01, -1.16767585e+00,  8.25597703e-01,\n",
       "       -5.77069938e-01,  3.41778666e-01,  1.16616654e+00,  1.51707911e+00,\n",
       "        5.88736713e-01, -3.98940206e-01, -4.64603961e-01, -8.20292175e-01,\n",
       "        1.79627329e-01,  2.33476734e+00, -1.28628445e+00, -1.61255836e-01,\n",
       "        1.95605516e+00, -8.56886029e-01, -7.99840331e-01, -5.42758226e-01,\n",
       "        8.05838823e-01, -5.55917263e-01, -2.43793225e+00,  5.63935935e-01,\n",
       "        6.52872443e-01, -1.23644555e+00, -6.70067668e-01,  1.66593301e+00,\n",
       "        9.89123464e-01, -7.95716822e-01,  8.20536673e-01, -1.28363025e+00,\n",
       "        1.36951149e-01,  1.15109658e+00,  3.09610635e-01, -1.13020968e+00,\n",
       "       -5.31014740e-01, -1.68677196e-01, -4.73654330e-01, -4.34276819e-01,\n",
       "       -3.11111957e-01, -1.56330764e+00,  1.08202136e+00,  5.37939489e-01,\n",
       "       -1.36778855e+00, -1.65561187e+00,  1.06077158e+00,  5.74272692e-01,\n",
       "        1.61319673e+00, -3.87682021e-01,  1.21888971e+00, -4.63775337e-01,\n",
       "        2.81570498e-02, -9.33641851e-01, -5.08571602e-02, -8.57643545e-01,\n",
       "       -1.35169327e+00, -9.49615121e-01, -5.79622209e-01,  2.18262529e+00,\n",
       "        1.41896188e+00, -4.46176082e-01, -8.39329898e-01,  6.31337702e-01,\n",
       "        2.01584983e+00, -3.27905327e-01,  6.98521435e-01, -7.30722070e-01,\n",
       "       -7.26184845e-01, -4.54636902e-01, -1.00361764e+00, -6.16510689e-01,\n",
       "        1.29322505e+00,  1.26797080e+00, -4.28777009e-01,  4.64685947e-01,\n",
       "        3.31737131e-01, -1.50880182e+00, -1.05266941e+00, -2.08112931e+00,\n",
       "       -7.28360236e-01, -5.59039235e-01,  2.92325288e-01, -3.42964053e-01,\n",
       "       -8.75920117e-01,  7.67681360e-01,  1.88823313e-01,  2.07405984e-01,\n",
       "        2.25870430e-01,  1.05813050e+00, -3.75994563e-01, -1.31466794e+00,\n",
       "       -3.03260963e-02, -4.01961237e-01,  4.67727423e-01, -5.66948473e-01,\n",
       "       -9.86766398e-01,  1.20020759e+00,  1.60733008e+00, -5.18875182e-01,\n",
       "        1.52654576e+00, -1.56614944e-01, -6.98875785e-01,  1.12777877e+00,\n",
       "        1.08789098e+00,  4.88000900e-01,  6.23857856e-01, -1.21310008e+00,\n",
       "       -1.63706347e-01,  4.04646307e-01, -7.27121294e-01, -2.72224933e-01,\n",
       "       -1.35332286e+00,  4.32050645e-01, -4.71763611e-01, -1.13438404e+00,\n",
       "        3.03746879e-01,  7.89678916e-02,  1.91097498e+00, -2.45133504e-01,\n",
       "        2.24047685e+00,  6.46981776e-01,  1.06327605e+00, -1.03101623e+00,\n",
       "        1.61626410e+00,  3.19654047e-02, -1.37142277e+00, -1.34819651e+00,\n",
       "       -3.65361184e-01,  1.52485287e+00,  4.27894980e-01,  1.21910298e+00,\n",
       "       -6.79543197e-01,  7.77000308e-01, -4.10969704e-02,  1.96144562e-02,\n",
       "       -1.40232456e+00,  1.64186692e+00, -5.50506823e-02,  2.28544876e-01,\n",
       "        2.51232594e-01,  4.44651037e-01, -5.88091254e-01, -3.39021564e-01,\n",
       "       -1.03868190e-02,  7.37265825e-01, -2.91938543e-01,  1.61380005e+00,\n",
       "       -7.27475584e-01,  1.86635643e-01, -1.00594449e+00, -1.97299242e-01,\n",
       "       -1.94591534e+00,  1.10918880e+00,  2.53965318e-01,  2.65511084e+00,\n",
       "       -6.26661703e-02, -7.09633112e-01, -2.87929702e+00,  1.74214423e+00,\n",
       "       -3.31732780e-01,  1.13223124e+00, -9.25872922e-01, -1.03493142e+00,\n",
       "       -1.61311746e-01, -1.39111644e-02,  1.96869180e-01,  4.21277493e-01,\n",
       "        5.63441992e-01, -2.31014824e+00, -1.36080718e+00, -8.45476210e-01,\n",
       "       -4.07770038e-01,  1.17051327e+00, -1.25552297e+00, -1.38148177e+00,\n",
       "        1.28931725e+00,  1.29293120e-02, -1.63268387e+00,  1.43088609e-01,\n",
       "        1.27672720e+00, -4.11586195e-01,  2.32319880e+00, -4.99174982e-01,\n",
       "        2.00337648e-01, -2.62801737e-01,  4.04021055e-01, -5.21461010e-01,\n",
       "       -1.36088681e+00,  5.19870043e-01,  5.92843533e-01, -5.12008429e-01,\n",
       "       -1.85476589e+00,  1.55348456e+00,  1.14155102e+00, -1.45963669e+00,\n",
       "       -1.58169019e+00,  7.97818661e-01,  2.44953521e-02, -1.52656555e+00,\n",
       "        1.44263804e+00, -6.18121266e-01, -2.72978634e-01, -4.31825876e-01,\n",
       "       -2.64501548e+00,  7.50126541e-01,  6.81878179e-02, -1.11094475e+00,\n",
       "       -4.13127035e-01, -7.10810304e-01,  1.01607585e+00, -1.24811351e+00,\n",
       "       -8.50533973e-03,  6.09791517e-01,  7.09727407e-02,  4.96780604e-01,\n",
       "       -8.90559137e-01, -1.18101561e+00,  1.70910098e-02, -7.35379577e-01,\n",
       "        3.29419434e-01,  9.48659778e-02, -3.38187313e+00,  4.18746948e-01,\n",
       "        1.06661463e+00, -1.73184443e+00, -1.29659569e+00,  9.50375795e-01,\n",
       "        2.06318542e-01, -1.08485818e+00,  1.66544735e-01, -1.65232611e+00,\n",
       "       -1.62903500e+00, -6.95285261e-01, -1.07978153e+00,  2.99073339e-01,\n",
       "        3.82494926e-01,  3.31809163e-01, -4.65521038e-01,  8.13322663e-01,\n",
       "        3.55792850e-01,  1.97934234e+00, -3.74845535e-01,  4.97624040e-01,\n",
       "        1.45491982e+00,  3.82772356e-01, -2.06641465e-01,  4.77358609e-01,\n",
       "       -1.77996957e+00,  2.03761458e+00,  3.03502560e-01,  1.61166096e+00,\n",
       "        7.34019697e-01, -6.77936196e-01,  9.45736349e-01, -9.51111093e-02,\n",
       "        1.56953645e+00,  2.59023428e-01,  1.59228468e+00,  1.71812087e-01,\n",
       "       -6.71805888e-02, -9.98983443e-01,  8.19324672e-01,  2.86718100e-01,\n",
       "       -1.62586021e+00,  1.55366158e+00, -7.92977691e-01,  6.63955584e-02,\n",
       "        1.68557382e+00,  5.30534498e-02,  1.16161287e+00,  1.98758399e+00,\n",
       "        6.57598615e-01,  1.15264583e+00, -3.06226969e-01, -1.11219235e-01,\n",
       "       -2.87337244e-01, -6.53393984e-01, -5.80733359e-01,  2.59501815e+00,\n",
       "       -1.40259460e-01,  1.52908432e+00, -3.25299710e-01,  8.96127403e-01,\n",
       "       -5.34764469e-01, -9.93357956e-01,  2.81122655e-01, -2.08350134e+00,\n",
       "       -1.04174483e+00, -2.89825082e-01,  2.16885614e+00,  1.37850082e+00,\n",
       "        7.65928328e-01, -7.51019120e-02, -4.62656021e-01, -4.74781841e-02,\n",
       "        6.21676296e-02,  4.82694626e-01,  1.31520748e+00, -1.80729735e+00,\n",
       "       -5.07668614e-01, -1.38031781e+00, -1.38060123e-01,  7.60879636e-01,\n",
       "        6.68297112e-01,  1.72829032e+00,  6.35762632e-01, -1.88298568e-01,\n",
       "       -2.18281969e-01, -4.84655321e-01,  3.93896282e-01,  1.17719877e+00,\n",
       "       -7.99971581e-01,  6.36096835e-01, -2.59077936e-01,  4.73809429e-02,\n",
       "       -1.38881505e+00, -2.05004722e-01,  1.02995181e+00,  1.24589288e+00,\n",
       "       -2.31886935e+00, -1.24913526e+00,  2.31966034e-01,  6.27453253e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputAnchor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 768)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputAnchor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(outputAnchor, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.4673 - accuracy: 0.9750 - val_loss: 1.6052 - val_accuracy: 0.4500\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4591 - accuracy: 0.9625 - val_loss: 1.6085 - val_accuracy: 0.5000\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.4488 - accuracy: 0.9625 - val_loss: 1.6086 - val_accuracy: 0.5000\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.4380 - accuracy: 0.9625 - val_loss: 1.6014 - val_accuracy: 0.4500\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4267 - accuracy: 0.9625 - val_loss: 1.5914 - val_accuracy: 0.4500\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.4209 - accuracy: 0.9750 - val_loss: 1.5874 - val_accuracy: 0.4500\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.4133 - accuracy: 0.9750 - val_loss: 1.5815 - val_accuracy: 0.4500\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4059 - accuracy: 0.9750 - val_loss: 1.5832 - val_accuracy: 0.4500\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.3969 - accuracy: 0.9625 - val_loss: 1.5858 - val_accuracy: 0.4500\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.3874 - accuracy: 0.9625 - val_loss: 1.5870 - val_accuracy: 0.4500\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.3802 - accuracy: 0.9750 - val_loss: 1.5877 - val_accuracy: 0.4500\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.3727 - accuracy: 0.9750 - val_loss: 1.5787 - val_accuracy: 0.4500\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.3667 - accuracy: 0.9750 - val_loss: 1.5685 - val_accuracy: 0.4500\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.3611 - accuracy: 0.9750 - val_loss: 1.5669 - val_accuracy: 0.4500\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.3534 - accuracy: 0.9750 - val_loss: 1.5596 - val_accuracy: 0.4500\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.3465 - accuracy: 0.9750 - val_loss: 1.5480 - val_accuracy: 0.4500\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.3404 - accuracy: 0.9750 - val_loss: 1.5453 - val_accuracy: 0.4500\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.3344 - accuracy: 0.9750 - val_loss: 1.5444 - val_accuracy: 0.4500\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.3277 - accuracy: 0.9750 - val_loss: 1.5418 - val_accuracy: 0.4500\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.3219 - accuracy: 0.9750 - val_loss: 1.5532 - val_accuracy: 0.4500\n",
      "Epoch 21/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.3161 - accuracy: 0.9750 - val_loss: 1.5562 - val_accuracy: 0.4500\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.3094 - accuracy: 0.9875 - val_loss: 1.5562 - val_accuracy: 0.5000\n",
      "Epoch 23/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.3040 - accuracy: 0.9875 - val_loss: 1.5537 - val_accuracy: 0.5000\n",
      "Epoch 24/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.3006 - accuracy: 0.9875 - val_loss: 1.5556 - val_accuracy: 0.5000\n",
      "Epoch 25/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.2953 - accuracy: 0.9875 - val_loss: 1.5562 - val_accuracy: 0.5000\n",
      "Epoch 26/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.2900 - accuracy: 0.9875 - val_loss: 1.5502 - val_accuracy: 0.5000\n",
      "Epoch 27/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.2839 - accuracy: 0.9875 - val_loss: 1.5478 - val_accuracy: 0.5000\n",
      "Epoch 28/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.2803 - accuracy: 0.9875 - val_loss: 1.5503 - val_accuracy: 0.5000\n",
      "Epoch 29/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.2764 - accuracy: 0.9875 - val_loss: 1.5592 - val_accuracy: 0.5000\n",
      "Epoch 30/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.2732 - accuracy: 0.9875 - val_loss: 1.5745 - val_accuracy: 0.5000\n",
      "Epoch 31/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.2673 - accuracy: 0.9875 - val_loss: 1.5716 - val_accuracy: 0.5000\n",
      "Epoch 32/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.2632 - accuracy: 0.9875 - val_loss: 1.5614 - val_accuracy: 0.5000\n",
      "Epoch 33/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.2585 - accuracy: 0.9875 - val_loss: 1.5468 - val_accuracy: 0.5000\n",
      "Epoch 34/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.2553 - accuracy: 0.9875 - val_loss: 1.5397 - val_accuracy: 0.5000\n",
      "Epoch 35/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.2507 - accuracy: 0.9875 - val_loss: 1.5457 - val_accuracy: 0.5000\n",
      "Epoch 36/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.2475 - accuracy: 0.9875 - val_loss: 1.5630 - val_accuracy: 0.5000\n",
      "Epoch 37/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.2434 - accuracy: 0.9875 - val_loss: 1.5717 - val_accuracy: 0.5000\n",
      "Epoch 38/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.2391 - accuracy: 0.9875 - val_loss: 1.5784 - val_accuracy: 0.5000\n",
      "Epoch 39/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.2357 - accuracy: 0.9875 - val_loss: 1.5795 - val_accuracy: 0.5000\n",
      "Epoch 40/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.2324 - accuracy: 0.9875 - val_loss: 1.5878 - val_accuracy: 0.5000\n",
      "Epoch 41/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.2292 - accuracy: 0.9875 - val_loss: 1.5795 - val_accuracy: 0.5000\n",
      "Epoch 42/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.2254 - accuracy: 0.9875 - val_loss: 1.5756 - val_accuracy: 0.5000\n",
      "Epoch 43/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.2219 - accuracy: 0.9875 - val_loss: 1.5694 - val_accuracy: 0.5000\n",
      "Epoch 44/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.2185 - accuracy: 0.9875 - val_loss: 1.5579 - val_accuracy: 0.5000\n",
      "Epoch 45/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.2159 - accuracy: 0.9875 - val_loss: 1.5486 - val_accuracy: 0.5000\n",
      "Epoch 46/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.2128 - accuracy: 0.9875 - val_loss: 1.5427 - val_accuracy: 0.5000\n",
      "Epoch 47/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.2104 - accuracy: 0.9875 - val_loss: 1.5472 - val_accuracy: 0.5000\n",
      "Epoch 48/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.2084 - accuracy: 0.9875 - val_loss: 1.5495 - val_accuracy: 0.5000\n",
      "Epoch 49/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.2046 - accuracy: 0.9875 - val_loss: 1.5620 - val_accuracy: 0.5000\n",
      "Epoch 50/50\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.2021 - accuracy: 0.9875 - val_loss: 1.5710 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27653ae8610>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Classifier_model.fit(x_train_flat,y_train_onehot, validation_data=(x_test_flat,y_test_onehot),epochs=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
