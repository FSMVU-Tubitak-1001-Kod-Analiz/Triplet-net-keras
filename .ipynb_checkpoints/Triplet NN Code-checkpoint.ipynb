{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import random\n",
    "import matplotlib.patheffects as PathEffects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv2D, Conv1D, Lambda, Dense, Flatten,MaxPooling2D, concatenate\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "# from tensorflow.keras.optimizers import SGD\n",
    "from keras.losses import binary_crossentropy\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_dataset(train_test_ratio):\n",
    "    X = []\n",
    "    y = []\n",
    "    X = pd.read_excel(io='CompareResult4.xlsx', sheet_name='Sheet1',usecols=\"J,K,L\")\n",
    "    X.fillna(0,inplace=True)\n",
    "    X=X.to_numpy()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train = get_train_test_dataset(0.5)\n",
    "\n",
    "\n",
    "\n",
    "#x_train=tf.convert_to_tensor(x_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our own plot function\n",
    "def scatter(x, labels, subtitle=None):\n",
    "    # We choose a color palette with seaborn.\n",
    "    palette = np.array(sns.color_palette(\"hls\", 10))\n",
    "\n",
    "    # We create a scatter plot.\n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40,\n",
    "                    c=palette[labels.astype(int)])\n",
    "    plt.xlim(-25, 25)\n",
    "    plt.ylim(-25, 25)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "\n",
    "    # We add the labels for each digit.\n",
    "    txts = []\n",
    "    for i in range(10):\n",
    "        # Position of each label.\n",
    "        xtext, ytext = np.median(x[labels == i, :], axis=0)\n",
    "        txt = ax.text(xtext, ytext, str(i), fontsize=24)\n",
    "        txt.set_path_effects([\n",
    "            PathEffects.Stroke(linewidth=5, foreground=\"w\"),\n",
    "            PathEffects.Normal()])\n",
    "        txts.append(txt)\n",
    "        \n",
    "    if subtitle != None:\n",
    "        plt.suptitle(subtitle)\n",
    "        \n",
    "    plt.savefig(subtitle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size=8396\n",
    "#x_train_flat_anchor = tf.reshape(x_train[0:,0], [data_size, 1])\n",
    "\n",
    "x_train_flat_anchor = x_train[0:,0]\n",
    "x_train_flat_positive = x_train[0:,1]\n",
    "x_train_flat_negative = x_train[0:,2]\n",
    "\n",
    "#print(x_train_flat_anchor)\n",
    "#x_test_flat = x_test.reshape(-1,784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tsne = TSNE()\n",
    "#train_tsne_embeds = tsne.fit_transform(x_train_flat[:512])\n",
    "#scatter(train_tsne_embeds, y_train[:512], \"Samples from Training Data\")\n",
    "\n",
    "#eval_tsne_embeds = tsne.fit_transform(x_test_flat[:512])\n",
    "#scatter(eval_tsne_embeds, y_test[:512], \"Samples from Validation Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Classifier_input = Input((784,))\n",
    "Classifier_output = Dense(10, activation='softmax')(Classifier_input)\n",
    "Classifier_model = Model(Classifier_input, Classifier_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train_onehot = le.fit_transform(y_train)\n",
    "#y_test_onehot = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Classifier_model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifier_model.fit(x_train_flat,y_train_onehot, validation_data=(x_test_flat,y_test_onehot),epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_triplet():\n",
    "    x= get_train_test_dataset(0.7);  \n",
    "                \n",
    "    return x;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = generate_triplet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8396, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triplet NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base_network(in_dims):\n",
    "    \"\"\"\n",
    "    Base network to be shared.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    #model.add(Input(tensor=tf_embedding_input))\n",
    "    #model.add(Flatten(input_shape=(1,1,1,))) #this line\n",
    "    #model.add(Conv1D(1,(7,7),padding='same',input_shape=(in_dims[0],in_dims[1],in_dims[2],),activation='relu',name='conv1'))\n",
    "    #model.add(MaxPooling2D((2,2),(2,2),padding='same',name='pool1'))\n",
    "    #model.add(Conv1D(10,(1),padding='same',activation='relu',name='conv2'))\n",
    "    #model.add(MaxPooling2D((2,2),(2,2),padding='same',name='pool2'))\n",
    "    #model.add(Flatten(name='flatten'))\n",
    "    #model.add(Dense(10,name='embeddings'))\n",
    "    # model.add(Dense(600))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ali.nizam\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "adam_optim = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 3)\n"
     ]
    }
   ],
   "source": [
    "data_size=1\n",
    "anchor_input = Input((1, ), name='anchor_input')\n",
    "positive_input = Input((1, ), name='positive_input')\n",
    "negative_input = Input((1, ), name='negative_input')\n",
    "\n",
    "# Shared embedding layer for positive and negative items\n",
    "Shared_DNN = create_base_network([1,])\n",
    "\n",
    "\n",
    "encoded_anchor = Shared_DNN(anchor_input)\n",
    "encoded_positive = Shared_DNN(positive_input)\n",
    "encoded_negative = Shared_DNN(negative_input)\n",
    "\n",
    "\n",
    "merged_vector = concatenate([encoded_anchor, encoded_positive, encoded_negative], axis=-1, name='merged_layer')\n",
    "print(merged_vector.shape)\n",
    "model = Model(inputs=[anchor_input,positive_input, negative_input], outputs=merged_vector)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow import keras\n",
    "#loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "#model.compile(loss=loss_fn, optimizer=adam_optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " anchor_input (InputLayer)      [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " positive_input (InputLayer)    [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " negative_input (InputLayer)    [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " sequential (Sequential)        multiple             0           ['anchor_input[0][0]',           \n",
      "                                                                  'positive_input[0][0]',         \n",
      "                                                                  'negative_input[0][0]']         \n",
      "                                                                                                  \n",
      " merged_layer (Concatenate)     (None, 3)            0           ['sequential[0][0]',             \n",
      "                                                                  'sequential[1][0]',             \n",
      "                                                                  'sequential[2][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8396\n",
      "8396\n"
     ]
    }
   ],
   "source": [
    "Anchor = x_train[:,0]\n",
    "Positive = x_train[:,1]\n",
    "Negative = x_train[:,2]\n",
    "Anchor_test = x_train[:,0]\n",
    "Positive_test = x_train[:,1]\n",
    "Negative_test = x_train[:,2]\n",
    "\n",
    "Y_dummy = np.empty((Anchor.shape[0],1))\n",
    "print(Y_dummy.size)\n",
    "Y_dummy2 = np.empty((Anchor_test.shape[0],1))\n",
    "print(Y_dummy2.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1, 1, 1, ..., 1, 1, 1], dtype=int64),\n",
       " array([2, 5, 5, ..., 2, 2, 2], dtype=int64),\n",
       " array([5, 5, 2, ..., 2, 2, 2], dtype=int64)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[Anchor,Positive,Negative]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "y_pred.shape =  Tensor(\"model_1/merged_layer/concat:0\", shape=(None, 3), dtype=float32)\n",
      "pos_dist =  Tensor(\"triplet_loss/Sum:0\", shape=(None,), dtype=float32)\n",
      "neg_dist =  Tensor(\"triplet_loss/Sum_1:0\", shape=(None,), dtype=float32)\n",
      "basic_loss =  Tensor(\"triplet_loss/add:0\", shape=(None,), dtype=float32)\n",
      "y_pred.shape =  Tensor(\"model_1/merged_layer/concat:0\", shape=(None, 3), dtype=float32)\n",
      "pos_dist =  Tensor(\"triplet_loss/Sum:0\", shape=(None,), dtype=float32)\n",
      "neg_dist =  Tensor(\"triplet_loss/Sum_1:0\", shape=(None,), dtype=float32)\n",
      "basic_loss =  Tensor(\"triplet_loss/add:0\", shape=(None,), dtype=float32)\n",
      "751/840 [=========================>....] - ETA: 0s - loss: 0.4019y_pred.shape =  Tensor(\"model_1/merged_layer/concat:0\", shape=(None, 3), dtype=float32)\n",
      "pos_dist =  Tensor(\"triplet_loss/Sum:0\", shape=(None,), dtype=float32)\n",
      "neg_dist =  Tensor(\"triplet_loss/Sum_1:0\", shape=(None,), dtype=float32)\n",
      "basic_loss =  Tensor(\"triplet_loss/add:0\", shape=(None,), dtype=float32)\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.4017 - val_loss: 0.4017\n",
      "Epoch 2/10\n",
      "840/840 [==============================] - 1s 1ms/step - loss: 0.4017 - val_loss: 0.4017\n",
      "Epoch 3/10\n",
      "840/840 [==============================] - 1s 1ms/step - loss: 0.4017 - val_loss: 0.4017\n",
      "Epoch 4/10\n",
      "840/840 [==============================] - 1s 1ms/step - loss: 0.4017 - val_loss: 0.4017\n",
      "Epoch 5/10\n",
      "840/840 [==============================] - 1s 1ms/step - loss: 0.4017 - val_loss: 0.4017\n",
      "Epoch 6/10\n",
      "840/840 [==============================] - 1s 1ms/step - loss: 0.4017 - val_loss: 0.4017\n",
      "Epoch 7/10\n",
      "840/840 [==============================] - 1s 1ms/step - loss: 0.4017 - val_loss: 0.4017\n",
      "Epoch 8/10\n",
      "840/840 [==============================] - 1s 1ms/step - loss: 0.4017 - val_loss: 0.4017\n",
      "Epoch 9/10\n",
      "840/840 [==============================] - 1s 1ms/step - loss: 0.4017 - val_loss: 0.4017\n",
      "Epoch 10/10\n",
      "840/840 [==============================] - 1s 1ms/step - loss: 0.4017 - val_loss: 0.4017\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25ae3e0fb20>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def triplet_loss(y_true, y_pred, alpha = 0.4):\n",
    "    \"\"\"\n",
    "    Implementation of the triplet loss function\n",
    "    Arguments:\n",
    "    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n",
    "    y_pred -- python list containing three objects:\n",
    "            anchor -- the encodings for the anchor data\n",
    "            positive -- the encodings for the positive data (similar to anchor)\n",
    "            negative -- the encodings for the negative data (different from anchor)\n",
    "    Returns:\n",
    "    loss -- real number, value of the loss\n",
    "    \"\"\"\n",
    "\n",
    "    import sys\n",
    "    print('y_pred.shape = ',y_pred)\n",
    "\n",
    "    total_lenght = y_pred.shape.as_list()[-1]\n",
    "#     print('total_lenght=',  total_lenght)\n",
    "#     total_lenght =12\n",
    "\n",
    "\n",
    "    anchor = y_pred[:,0:int(total_lenght*1/3)]\n",
    "    positive = y_pred[:,int(total_lenght*1/3):int(total_lenght*2/3)]\n",
    "    negative = y_pred[:,int(total_lenght*2/3):int(total_lenght*3/3)]\n",
    "\n",
    "\n",
    "    # distance between the anchor and the positive\n",
    "    pos_dist = K.sum(K.square(anchor-positive),axis=1)\n",
    "    print('pos_dist = ',pos_dist)\n",
    "\n",
    "    # distance between the anchor and the negative\n",
    "    neg_dist = K.sum(K.square(anchor-negative),axis=1)\n",
    "\n",
    "    print('neg_dist = ',neg_dist)\n",
    "\n",
    "    # compute loss\n",
    "    basic_loss = pos_dist-neg_dist+alpha\n",
    "    print('basic_loss = ',basic_loss)\n",
    "\n",
    "    \"\"\"\n",
    "        basic_loss = pos_dist-neg_dist+alpha\n",
    "        Node: 'triplet_loss/add'\n",
    "        Incompatible shapes: [10,3] vs. [10]\n",
    "\t    [[{{node triplet_loss/add}}]] [Op:__inference_train_function_364]\n",
    "    \"\"\"\n",
    "\n",
    "    loss = K.maximum(basic_loss,0.0)\n",
    "\n",
    "    #return tf.constant(loss.data[0])\n",
    "    #return tf.constant(0)\n",
    "    return loss\n",
    "\n",
    "model.compile(loss=triplet_loss, optimizer=adam_optim)\n",
    "\n",
    "\n",
    "model.fit([Anchor,Positive,Negative],y=Y_dummy,validation_data=([Anchor_test,Positive_test,Negative_test],Y_dummy2), batch_size=10, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = Model(inputs=anchor_input, outputs=encoded_anchor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x25ae0963b80>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trained_model.load_weights('triplet_model_MNIST.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ali.nizam\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\manifold\\_t_sne.py:780: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ali.nizam\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\manifold\\_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tsne = TSNE()\n",
    "X_train_trm = trained_model.predict(x_train[:512].reshape(-1,1))\n",
    "# X_test_trm = trained_model.predict(x_test[:512].reshape(-1,28,28,1))\n",
    "train_tsne_embeds = tsne.fit_transform(X_train_trm)\n",
    "# eval_tsne_embeds = tsne.fit_transform(X_test_trm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 20.70862   ,  -0.8581075 ],\n",
       "       [-12.627098  ,   0.17043948],\n",
       "       [ -8.522605  ,   0.704525  ],\n",
       "       ...,\n",
       "       [ 20.722538  ,  -0.8586259 ],\n",
       "       [ -9.947902  ,   0.8998189 ],\n",
       "       [-10.34355   ,  -0.28967533]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tsne_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter(train_tsne_embeds, y_train[:512], \"Training Data After TNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter(eval_tsne_embeds, y_test[:512], \"Validation Data After TNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_trm = trained_model.predict(x_train.reshape(-1,28,28,1))\n",
    "# X_test_trm = trained_model.predict(x_test.reshape(-1,28,28,1))\n",
    "#\n",
    "# Classifier_input = Input((4,))\n",
    "# Classifier_output = Dense(10, activation='softmax')(Classifier_input)\n",
    "# Classifier_model = Model(Classifier_input, Classifier_output)\n",
    "#\n",
    "#\n",
    "# Classifier_model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "#\n",
    "# Classifier_model.fit(X_train_trm,y_train_onehot, validation_data=(X_test_trm,y_test_onehot),epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
